{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e11facad-c2f7-41ae-8723-a20790f56253",
   "metadata": {},
   "source": [
    "## Yin Attack (InvertingGradients) on CIFAR-100\n",
    "**Objective:** Reconstruct training data from gradients captured during federated learning simulation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f99258-d95d-4a9b-9a76-14ce8222ed2a",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "335f7bfb-ae7e-4b18-84d4-382fca626635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import breaching\n",
    "import breaching.attacks as attacks\n",
    "import breaching.cases as cases\n",
    "import breaching.utils as utils\n",
    "import torchvision.models as models\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ccf515-0c92-4c50-a34d-533ead8a8398",
   "metadata": {},
   "source": [
    "### 2. Final Attack Configuration\n",
    "This configuration is optimized for the highest possible fidelity, addressing the stability issues (LR), noise/blur (TV), and the architectural defense of ResNet18 (BN Stats).\n",
    "\n",
    "| Parameter | Value | Reason |\n",
    "| :--- | :--- | :--- |\n",
    "| **Attack** | `invertinggradients` | The \"successful\" reconstruction method. |\n",
    "| **Target Model** | ResNet18 | Model architecture. |\n",
    "| **Dataset** | CIFAR-100 |  Target dataset. |\n",
    "| **Batch Size** | 1 | Essential for perfect gradient inversion. |\n",
    "| **Device** | GPU (CUDA) | Used for speed. |\n",
    "| **Max Iterations** | $\\mathbf{20,000}$ | Sufficient time for convergence. |\n",
    "| **Learning Rate (LR)** | $\\mathbf{0.001}$ | Ensures **stability** and high precision. |\n",
    "| **Total Variation (TV)** | $\\mathbf{1e-2}$ | Enforces **sharpness** and smoothness. |\n",
    "| **Critical Fix** | $\\mathbf{+reconstruct\\_bn\\_statistics=True}$ | **Bypasses the Batch Norm defense** for clear reconstruction. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb97a96c-eb1a-4bee-a123-355c290c70d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/rbhandar/.local/lib/python3.12/site-packages/breaching/__init__.py:18: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  with hydra.initialize(config_path=\"config\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Investigating use case single_imagenet with server type honest_but_curious.\n",
      "Attack type: invertinggradients\n",
      "Attack learning rate: 0.001\n",
      "Attack max iterations: 20000\n",
      "Total Variation Reg: 0.01\n"
     ]
    }
   ],
   "source": [
    "cfg = breaching.get_config(overrides = [\"attack=invertinggradients\",\n",
    "                                        \"+optim.max_iterations=20000\", \n",
    "                                        \"+optim.lr=0.001\",  # learning rate\n",
    "                                        \"+regularization.total_variation=1e-2\", # TV penalty\n",
    "                                        \"+reconstruct_bn_statistics=True\" # batch normalization\n",
    "                                       ])\n",
    "\n",
    "\n",
    "print(f\"Attack type: {cfg.attack.type}\")\n",
    "\n",
    "# The parameters are now created under the top-level config (cfg)\n",
    "if 'optim' in cfg and 'regularization' in cfg:\n",
    "    print(f\"Attack learning rate: {cfg.optim.lr}\")\n",
    "    print(f\"Attack max iterations: {cfg.optim.max_iterations}\")\n",
    "    print(f\"Total Variation Reg: {cfg.regularization.total_variation}\")\n",
    "else:\n",
    "    print(\"Configuration successful, but keys are nested differently. Proceeding to attack.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d810f443-1eeb-4363-b0f5-df0fe07c5acf",
   "metadata": {},
   "source": [
    "## 3. Load FL Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "755d5875-b694-4425-9ffe-f92fb34fe7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: fedavg_metrics_04_tensors.pt\n",
      "Keys: ['global_model_state', 'raw_gradients', 'model_updates', 'server_aggregate_delta']\n",
      "Clients: [2, 11, 13, 3, 10]\n",
      "Client 2: 1 gradient steps\n",
      "Parameters: 62\n"
     ]
    }
   ],
   "source": [
    "# Load saved gradients from FL simulation\n",
    "gradient_dir = Path(\"/scratch/project_2015432/Sec_FL_ritesh/src/fl_simulation/reports/fedavg_baseline/round_04\")\n",
    "gradient_files = sorted(gradient_dir.glob(\"fedavg_metrics_*_tensors.pt\"))\n",
    "grad_file = gradient_files[0]\n",
    "gradient_data = torch.load(grad_file, map_location='cpu')\n",
    "\n",
    "print(f\"Loaded: {grad_file.name}\")\n",
    "print(f\"Keys: {list(gradient_data.keys())}\")\n",
    "print(f\"Clients: {list(gradient_data['raw_gradients'].keys())}\")\n",
    "\n",
    "# Check first client\n",
    "client_id = list(gradient_data['raw_gradients'].keys())[0]\n",
    "client_data = gradient_data['raw_gradients'][client_id]\n",
    "num_steps = len(client_data['grads_per_step_raw'])\n",
    "\n",
    "print(f\"Client {client_id}: {num_steps} gradient steps\")\n",
    "print(f\"Parameters: {len(client_data['grads_per_step_raw'][0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c0e25e-4fb4-4e91-bb0a-661becfd619b",
   "metadata": {},
   "source": [
    "## 4. Load Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e0159a5-67bd-44e9-8ae4-0bfa2a694f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/rbhandar/.local/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/users/rbhandar/.local/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is loaded\n"
     ]
    }
   ],
   "source": [
    "model_state = gradient_data['global_model_state']\n",
    "model = models.resnet18(pretrained=False)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 100)  # CIFAR-100 has 100 classes\n",
    "model.load_state_dict(model_state, strict=False)\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "setup = dict(device=torch.device('cuda'), dtype=torch.float32)\n",
    "attacker = breaching.attacks.prepare_attack(model, loss_fn, cfg.attack, setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4ec454-d2d6-49e0-9c6b-29c1a6ec82e4",
   "metadata": {},
   "source": [
    "## 5. Extract Client Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be8e1a95-1952-42f5-8897-fc972c1c4bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client: 2\n",
      "Number of gradient tensors: 62\n"
     ]
    }
   ],
   "source": [
    "# Get gradients from first client, first training step\n",
    "client_id = list(gradient_data['raw_gradients'].keys())[0]\n",
    "client_grads = gradient_data['raw_gradients'][client_id]\n",
    "grad_dict = client_grads['grads_per_step_raw'][0]\n",
    "\n",
    "print(f\"Client: {client_id}\")\n",
    "print(f\"Number of gradient tensors: {len(grad_dict)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93558a62-1c0f-402e-bb4b-69e1448cd951",
   "metadata": {},
   "source": [
    "## 6. Prepare Attack Inputs: Gradients and Model State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7599d13-2c56-44f8-a434-4645b233ec98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 62\n",
      "Buffers: 60\n",
      "Reordered 62 gradients to match model\n",
      "\n",
      "Verifying alignment:\n",
      "All shapes match!\n",
      "server_payload and shared_data created successfully!\n",
      "Using label=0 as initial guess (may not be the actual label)\n"
     ]
    }
   ],
   "source": [
    "# Define CIFAR-100 metadata and separate model parameters/buffers\n",
    "class DataConfig:\n",
    "    modality = \"vision\"\n",
    "    size = (50_000,)\n",
    "    classes = 100\n",
    "    shape = (3, 32, 32)\n",
    "    normalize = True\n",
    "    mean = (0.5071, 0.4867, 0.4408)\n",
    "    std = (0.2675, 0.2565, 0.2761)\n",
    "\n",
    "data_cfg = DataConfig()\n",
    "\n",
    "parameters = []\n",
    "buffers = []\n",
    "for name, tensor in gradient_data['global_model_state'].items():\n",
    "    if 'running' in name or 'num_batches' in name:\n",
    "        buffers.append(tensor.cuda())\n",
    "    else:\n",
    "        parameters.append(tensor.cuda())\n",
    "\n",
    "server_payload = [dict(\n",
    "    parameters=parameters,\n",
    "    buffers=buffers,\n",
    "    metadata=data_cfg\n",
    ")]\n",
    "\n",
    "print(f\"Parameters: {len(parameters)}\")\n",
    "print(f\"Buffers: {len(buffers)}\")\n",
    "\n",
    "# Create gradients_ordered \n",
    "grad_dict = client_grads['grads_per_step_raw'][0]\n",
    "gradients_ordered = []\n",
    "for name, param in model.named_parameters():\n",
    "    if name in grad_dict:\n",
    "        gradients_ordered.append(grad_dict[name].cuda())\n",
    "    else:\n",
    "        print(f\"Warning: {name} not found in gradients\")\n",
    "\n",
    "print(f\"Reordered {len(gradients_ordered)} gradients to match model\")\n",
    "\n",
    "# Verify shapes match\n",
    "print(\"\\nVerifying alignment:\")\n",
    "for i, (g, p) in enumerate(zip(gradients_ordered, model.parameters())):\n",
    "    if g.shape != p.shape:\n",
    "        print(f\"Mismatch at {i}: {g.shape} vs {p.shape}\")\n",
    "        break\n",
    "else:\n",
    "    print(\"All shapes match!\")\n",
    "\n",
    "# Create shared_data - using label 0 as initial guess\n",
    "shared_data = [\n",
    "    dict(\n",
    "        gradients=gradients_ordered,\n",
    "        buffers=None,\n",
    "        metadata=dict(  \n",
    "            num_data_points=1,\n",
    "            labels=torch.tensor([0]).cuda(),  # Starting guess\n",
    "            local_hyperparams=dict(\n",
    "                lr=0.01,\n",
    "                momentum=0.0,\n",
    "                weight_decay=0.0,\n",
    "                steps=1,\n",
    "                data_per_step=1,\n",
    "                labels=[torch.tensor([0]).cuda()],  # Also needed here!\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"server_payload and shared_data created successfully!\")\n",
    "print(\"Using label=0 as initial guess (may not be the actual label)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cba8d0-3ae4-4668-9ef9-edfacb5c8744",
   "metadata": {},
   "source": [
    "## 7. Execute Gradient Inversion Attack\n",
    "**Warning:** This may take several minutes on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61da0ef4-6042-41a3-94d9-b866a9dbf751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Yin attack (20,000 iterations)...\n",
      "This will take 10-20 minutes...\n",
      "\n",
      "Attack completed!\n",
      "Keys: ['data', 'labels']\n",
      "Reconstructed data shape: torch.Size([1, 3, 32, 32])\n",
      "Reconstructed labels: tensor([0], device='cuda:0')\n",
      "reconstructed_img variable is ready for visualization\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 - Execute Attack (FINAL CORRECTED VERSION)\n",
    "\n",
    "print(\"Running Yin attack (20,000 iterations)...\")\n",
    "print(\"This will take 10-20 minutes...\")\n",
    "\n",
    "reconstructed_user_data, stats = attacker.reconstruct(\n",
    "    server_payload, \n",
    "    shared_data,        \n",
    "    server_secrets={},\n",
    "    dryrun=False  \n",
    ")\n",
    "\n",
    "print(\"\\nAttack completed!\")\n",
    "print(f\"Keys: {list(reconstructed_user_data.keys())}\")\n",
    "print(f\"Reconstructed data shape: {reconstructed_user_data['data'].shape}\")\n",
    "print(f\"Reconstructed labels: {reconstructed_user_data['labels']}\")\n",
    "\n",
    "# Extract image from the dictionary\n",
    "reconstructed_img = reconstructed_user_data['data'][0].cpu()  # Get first (and only) image\n",
    "print(\"reconstructed_img variable is ready for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b94aa1-e2c9-4372-80e1-5123d9053079",
   "metadata": {},
   "source": [
    "## 8. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e70832a0-6925-4571-bec6-9978f2d8956a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed data shape: torch.Size([1, 3, 32, 32])\n",
      "Reconstructed labels: tensor([0], device='cuda:0')\n",
      "\n",
      "Image stats:\n",
      "Min pixel value: -1.8974658250808716\n",
      "Max pixel value: 2.025352954864502\n",
      "Mean: 0.08886990696191788\n"
     ]
    }
   ],
   "source": [
    "# Check what was reconstructed\n",
    "print(\"Reconstructed data shape:\", reconstructed_user_data['data'].shape)\n",
    "print(\"Reconstructed labels:\", reconstructed_user_data['labels'])\n",
    "\n",
    "\n",
    "# Also show some stats\n",
    "print(f\"\\nImage stats:\")\n",
    "print(f\"Min pixel value: {reconstructed_img.min()}\")\n",
    "print(f\"Max pixel value: {reconstructed_img.max()}\")\n",
    "print(f\"Mean: {reconstructed_img.mean()}\")\n",
    "\n",
    "\n",
    "# The stats dictionary usually contains a list of loss values per iteration\n",
    "if 'final_loss' in stats:\n",
    "    print(f\"Final Loss Value (Total): {stats['final_loss']}\")\n",
    "elif 'loss_list' in stats:\n",
    "    # If 'final_loss' isn't explicitly stored, take the last value from the list\n",
    "    print(f\"Final Loss Value (Last Iteration): {stats['loss_list'][-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f42c4920-9593-4b05-aacf-fd874e1b6a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Attack Results ===\n",
      "Reconstructed data shape: torch.Size([1, 3, 32, 32])\n",
      "Recovered label: 0\n",
      "\n",
      "Image stats (before denormalization):\n",
      "  Min: -1.8975\n",
      "  Max: 2.0254\n",
      "  Mean: 0.0889\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Attack Results: \")\n",
    "print(f\"Reconstructed data shape: {reconstructed_user_data['data'].shape}\")\n",
    "print(f\"Recovered label: {reconstructed_user_data['labels'].item()}\")\n",
    "\n",
    "# Image stats\n",
    "print(f\"\\nImage stats (before denormalization):\")\n",
    "print(f\"  Min: {reconstructed_img.min():.4f}\")\n",
    "print(f\"  Max: {reconstructed_img.max():.4f}\")\n",
    "print(f\"  Mean: {reconstructed_img.mean():.4f}\")\n",
    "\n",
    "# Check if stats dict has useful info\n",
    "if 'opt' in stats:\n",
    "    print(f\"\\nFinal optimization loss: {stats['opt']:.6f}\")\n",
    "\n",
    "# Denormalize and visualize\n",
    "img = reconstructed_img.detach().clone()\n",
    "img = img.permute(1, 2, 0)  # (C, H, W) -> (H, W, C)\n",
    "\n",
    "# Denormalize\n",
    "mean = torch.tensor((0.5071, 0.4867, 0.4408))\n",
    "std = torch.tensor((0.2675, 0.2565, 0.2761))\n",
    "img = img * std + mean\n",
    "\n",
    "# Clip to valid range\n",
    "img = torch.clamp(img, 0, 1)\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(img.numpy())\n",
    "plt.title(f\"Reconstructed Image\\nRecovered Label: {reconstructed_user_data['labels'].item()}\")\n",
    "plt.axis('off')\n",
    "plt.savefig('reconstructed_final.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
