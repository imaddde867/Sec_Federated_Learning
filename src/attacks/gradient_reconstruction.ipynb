{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46c6da7a-e838-4794-9124-722a2f96c400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA cache cleared. Rerun your attack now.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "335f7bfb-ae7e-4b18-84d4-382fca626635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import breaching\n",
    "import torchvision.models as models\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb97a96c-eb1a-4bee-a123-355c290c70d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/rbhandar/.local/lib/python3.12/site-packages/breaching/__init__.py:18: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  with hydra.initialize(config_path=\"config\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Investigating use case single_imagenet with server type honest_but_curious.\n",
      "Attack type: {'see-through-gradients'}\n"
     ]
    }
   ],
   "source": [
    "# Configure Breaching for Yin (See-Through-Gradients) attack\n",
    "cfg = breaching.get_config(overrides=[\"attack=seethroughgradients\"])\n",
    "print(f\"Attack type:\", {cfg.attack.type})\n",
    "cfg.attack.regularization = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d8ed0bd-c06b-4712-a320-8ef3af5138fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.attack.optim.max_iterations = 5000  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "755d5875-b694-4425-9ffe-f92fb34fe7ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 31.74 GiB of which 7.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 29.89 GiB is allocated by PyTorch, and 1.54 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m gradient_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(gradient_dir\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfedavg_metrics_*_tensors.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      4\u001b[0m grad_file \u001b[38;5;241m=\u001b[39m gradient_files[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m gradient_data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/serialization.py:1516\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m   1515\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1516\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1517\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1518\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1519\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_weights_only_unpickler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1520\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1523\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1524\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/serialization.py:2114\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   2112\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[1;32m   2113\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[0;32m-> 2114\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2115\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2117\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/_weights_only_unpickler.py:532\u001b[0m, in \u001b[0;36mUnpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;28mtype\u001b[39m(pid) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pid) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mserialization\u001b[38;5;241m.\u001b[39m_maybe_decode_ascii(pid[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m     ):\n\u001b[1;32m    529\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\n\u001b[1;32m    530\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly persistent_load of storage is allowed, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpid[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    531\u001b[0m         )\n\u001b[0;32m--> 532\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpersistent_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [BINGET[\u001b[38;5;241m0\u001b[39m], LONG_BINGET[\u001b[38;5;241m0\u001b[39m]]:\n\u001b[1;32m    534\u001b[0m     idx \u001b[38;5;241m=\u001b[39m (read(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m BINGET[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m unpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<I\u001b[39m\u001b[38;5;124m\"\u001b[39m, read(\u001b[38;5;241m4\u001b[39m)))[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/serialization.py:2078\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   2076\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2077\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 2078\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2079\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2082\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/serialization.py:2044\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   2041\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mdetect_fake_mode(\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2044\u001b[0m     wrap_storage \u001b[38;5;241m=\u001b[39m \u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2045\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2046\u001b[0m     storage\u001b[38;5;241m.\u001b[39m_fake_device \u001b[38;5;241m=\u001b[39m location\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/serialization.py:1854\u001b[0m, in \u001b[0;36m_get_restore_location.<locals>.restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m   1853\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrestore_location\u001b[39m(storage, location):\n\u001b[0;32m-> 1854\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdefault_restore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/serialization.py:698\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;124;03mRestores `storage` using a deserializer function registered for the `location`.\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;124;03m       all matching ones return `None`.\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 698\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    700\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/serialization.py:637\u001b[0m, in \u001b[0;36m_deserialize\u001b[0;34m(backend_name, obj, location)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(backend_name):\n\u001b[1;32m    636\u001b[0m     device \u001b[38;5;241m=\u001b[39m _validate_device(location, backend_name)\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/storage.py:287\u001b[0m, in \u001b[0;36m_StorageBase.to\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, torch\u001b[38;5;241m.\u001b[39mdevice):\n\u001b[1;32m    286\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(device)\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_to\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/_utils.py:101\u001b[0m, in \u001b[0;36m_to\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_sparse, (\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse storage is not supported for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m     )\n\u001b[0;32m--> 101\u001b[0m     untyped_storage \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     untyped_storage\u001b[38;5;241m.\u001b[39mcopy_(\u001b[38;5;28mself\u001b[39m, non_blocking)\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m untyped_storage\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 31.74 GiB of which 7.62 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 29.89 GiB is allocated by PyTorch, and 1.54 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Load saved gradients from FL simulation\n",
    "gradient_dir = Path(\"/scratch/project_2015432/Sec_FL_ritesh/src/fl_simulation\")\n",
    "gradient_files = sorted(gradient_dir.glob(\"fedavg_metrics_*_tensors.pt\"))\n",
    "grad_file = gradient_files[0]\n",
    "gradient_data = torch.load(grad_file, map_location='cuda:2')\n",
    "print(\"Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee961fc-021b-4b2a-b38b-3ea98c0227d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee80977a-d48d-4bdb-bf1e-113dc89b156d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0159a5-67bd-44e9-8ae4-0bfa2a694f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state = gradient_data['global_model_state']\n",
    "model = models.resnet18(pretrained=False)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 100)  # CIFAR-100 has 100 classes\n",
    "model.load_state_dict(model_state, strict=False)\n",
    "model.eval()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "setup = dict(device=torch.device('cpu'), dtype=torch.float32)\n",
    "attacker = breaching.attacks.prepare_attack(model, loss_fn, cfg.attack, setup)\n",
    "print(\" Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8e1a95-1952-42f5-8897-fc972c1c4bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gradients from first client, first training step\n",
    "client_id = list(gradient_data['raw_gradients'].keys())[0]\n",
    "client_grads = gradient_data['raw_gradients'][client_id]\n",
    "grad_dict = client_grads['grads_per_step_raw'][0]\n",
    "gradients = [grad_dict[key] for key in sorted(grad_dict.keys())]\n",
    "print(f\"Client: {client_id}\")\n",
    "print(f\"Number of gradient tensors: {len(gradients)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7599d13-2c56-44f8-a434-4645b233ec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CIFAR-100 metadata and separate model parameters/buffers\n",
    "class DataConfig:\n",
    "    modality = \"vision\"\n",
    "    size = (50_000,)\n",
    "    classes = 100\n",
    "    shape = (3, 32, 32)\n",
    "    normalize = True\n",
    "    mean = (0.5071, 0.4867, 0.4408)\n",
    "    std = (0.2675, 0.2565, 0.2761)\n",
    "\n",
    "data_cfg = DataConfig()\n",
    "\n",
    "parameters = []\n",
    "buffers = []\n",
    "for name, tensor in gradient_data['global_model_state'].items():\n",
    "    if 'running' in name or 'num_batches' in name:\n",
    "        buffers.append(tensor)\n",
    "    else:\n",
    "        parameters.append(tensor)\n",
    "\n",
    "print(f\"Parameters: {len(parameters)}\")\n",
    "print(f\"Buffers: {len(buffers)}\")\n",
    "\n",
    "# Format data for Breaching API (server's view)\n",
    "server_payload = [dict(\n",
    "    parameters=parameters,\n",
    "    buffers=buffers,\n",
    "    metadata=data_cfg\n",
    ")]\n",
    "\n",
    "# Format data for Breaching API (attacker's view)\n",
    "shared_data = [dict(\n",
    "    gradients=gradients,\n",
    "    buffers=None,\n",
    "    metadata=dict(\n",
    "        num_data_points=1,\n",
    "        labels=torch.tensor([0]),  # Dummy label\n",
    "        local_hyperparams=None\n",
    "    )\n",
    ")]\n",
    "\n",
    "print(\" Data prepared with dummy label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73a94ec-d880-4bf2-b1fc-44b5d6416505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder gradients to match model parameter order (critical fix)\n",
    "grad_dict = client_grads['grads_per_step_raw'][0]\n",
    "\n",
    "gradients_ordered = []\n",
    "for name, param in model.named_parameters():\n",
    "    if name in grad_dict:\n",
    "        gradients_ordered.append(grad_dict[name])\n",
    "    else:\n",
    "        print(f\"Warning: {name} not found in gradients\")\n",
    "\n",
    "print(f\" Reordered {len(gradients_ordered)} gradients to match model\")\n",
    "\n",
    "# Verify shapes match\n",
    "print(\"\\nVerifying alignment:\")\n",
    "for i, (g, p) in enumerate(zip(gradients_ordered, model.parameters())):\n",
    "    if g.shape != p.shape:\n",
    "        print(f\"  Still mismatch at {i}: {g.shape} vs {p.shape}\")\n",
    "        break\n",
    "else:\n",
    "    print(\"All shapes match!\")\n",
    "\n",
    "# gradients and label\n",
    "shared_data = [\n",
    "    dict(\n",
    "        gradients=gradients_ordered,\n",
    "        buffers=None,\n",
    "        metadata=dict(\n",
    "            num_data_points=1,\n",
    "            labels=torch.tensor([0]),  # Top-level labels\n",
    "            local_hyperparams=dict(\n",
    "                lr=0.1,\n",
    "                momentum=0.9,\n",
    "                weight_decay=0.0,\n",
    "                steps=1,\n",
    "                data_per_step=32,\n",
    "                labels=[torch.tensor([0])]  # Labels list for each step\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "]\n",
    "print(\"shared_data updated with correctly ordered gradients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61da0ef4-6042-41a3-94d9-b866a9dbf751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute gradient inversion attack (takes 5-15 minutes)\n",
    "print(\"Running Yin attack...\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "reconstructed_user_data, stats = attacker.reconstruct(\n",
    "    server_payload,\n",
    "    shared_data,\n",
    "    server_secrets={},\n",
    "    dryrun=False\n",
    ")\n",
    "\n",
    "print(\"\\n Attack completed!\")\n",
    "print(f\"Reconstructed keys: {list(reconstructed_user_data.keys())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
