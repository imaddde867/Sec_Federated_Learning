{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "902ce877-c796-4abd-91e0-dd5cdad766f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libs and quick parameters definition\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "number_of_clients = 1\n",
    "number_of_rounds = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9418489-d662-454c-a872-e7a80b628b46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_cifar100():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(), #convert each image to a tensor\n",
    "        transforms.Normalize((0.5071, 0.4867, 0.4408),\n",
    "                             (0.2675, 0.2565, 0.2761))  # CIFAR-100 mean/std deviation\n",
    "    ])\n",
    "    train = datasets.CIFAR100(root=\"./data\", train=True, download=True, transform=transform)\n",
    "    test = datasets.CIFAR100(root=\"./data\", train=False, download=True, transform=transform)\n",
    "    return train, test\n",
    "\n",
    "train, test = load_cifar100()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cbcb6f-1e17-448e-98fc-083c0621dd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we try to simulate multiple clients in FL, each with a portion of the dataset\n",
    "# mode: \"iid\"，  \"dirichlet\"\n",
    "# alpha: Dirichlet concentration parameter, smaller values ​​result in more uneven distribution (0.5 or 0.1)\n",
    "def partition_dataset(dataset, num_clients, mode = \"iid\", alpha = 0.5):\n",
    "    n = len(dataset)\n",
    "    all_idx = np.arange(n)\n",
    "\n",
    "    if mode == \"iid\":  \n",
    "        idx = all_idx.tolist()\n",
    "        random.shuffle(idx) # shuffle the order to ensure randomness\n",
    "        split = n // num_clients\n",
    "        parts = [idx[i*split:(i+1)*split] for i in range(num_clients)]\n",
    "        # last client gets remainder\n",
    "        parts[-1].extend(idx[num_clients*split:])\n",
    "        return parts\n",
    "    else:\n",
    "        # non-iid: each client randomly receives a different number of samples\n",
    "        assert mode == \"dirichlet\", \"mode must be 'iid' or 'dirichlet'\"  \n",
    "\n",
    "        # get labels (common .targets / .labels in torchvision)\n",
    "        targets = getattr(dataset, \"targets\", None)\n",
    "        if targets is None:\n",
    "            targets = getattr(dataset, \"labels\", None)  \n",
    "        if torch.is_tensor(targets):                  \n",
    "            targets = targets.numpy()\n",
    "        targets = np.asarray(targets)\n",
    "        num_classes = int(targets.max() + 1)          \n",
    "\n",
    "        # calculate Dirichlet-based client shares for each class\n",
    "        parts = [[] for _ in range(num_clients)]       \n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "        for c in range(num_classes):\n",
    "            cls_idx = np.where(targets == c)[0]\n",
    "            rng.shuffle(cls_idx)\n",
    "\n",
    "            # one Dirichlet allocation per class\n",
    "            # smaller alpha -> more non-IID (more biased towards certain clients)\n",
    "            p = rng.dirichlet(alpha * np.ones(num_clients))   \n",
    "\n",
    "            # allocate integer quotas proportionally\n",
    "            raw = p * len(cls_idx)\n",
    "            sizes = raw.astype(int)\n",
    "\n",
    "            # because rounding will lose samples, fill in the remainder according to the maximum residual\n",
    "            residue = len(cls_idx) - sizes.sum()\n",
    "            if residue > 0:\n",
    "                # fill in the decimal part from large to small\n",
    "                fracs = raw - sizes                              \n",
    "                order = np.argsort(-fracs)\n",
    "                for j in order[:residue]:\n",
    "                    sizes[j] += 1\n",
    "\n",
    "            # split the index by sizes  (do this INSIDE the class loop)\n",
    "            start = 0\n",
    "            for j, need in enumerate(sizes):                     \n",
    "                if need > 0:\n",
    "                    parts[j].extend(cls_idx[start:start+need].tolist())\n",
    "                    start += need\n",
    "\n",
    "        # optional: shuffle each client's indices\n",
    "        for j in range(num_clients):\n",
    "            random.shuffle(parts[j])\n",
    "\n",
    "        return parts  # returns a list of lists, where each inner list contains dataset indices belonging to one client.\n",
    "\n",
    "# α = 0.5 (medium non-IID)\n",
    "parts = partition_dataset(train, number_of_clients, mode=\"dirichlet\", alpha=0.5)\n",
    "# α = 0.1 (strong non-IID)\n",
    "parts = partition_dataset(train, number_of_clients, mode=\"dirichlet\", alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6aba0617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1/1 finished. participants=1/1\n",
      "Global test acc: 0.2106\n",
      "Round 1/1 finished. participants=1/1\n",
      "Global test acc: 0.2134\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=100, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def local_train(model, dataset, device, epochs=1, batch_size=32, lr=0.01):\n",
    "    local_model = copy.deepcopy(model)  # each client gets its own copy of the global model\n",
    "    local_model.to(device)\n",
    "    local_model.train() # Train it on its local partition of the dataset\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    opt = optim.SGD(local_model.parameters(), lr=lr, momentum=0.9)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            opt.zero_grad()\n",
    "            out = local_model(x)\n",
    "            loss = loss_fn(out, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "    return local_model.state_dict()\n",
    "   # then after training we return only the trained weights : state_dict\n",
    "\n",
    "# FedAvg algorithm (weighted by client sample counts)\n",
    "def average_weights(weight_list, sizes):# weighted average based on the number of samples from each client\n",
    "    assert len(weight_list) == len(sizes) and len(weight_list) >0\n",
    "    total = float(sum(sizes))\n",
    "    avg = copy.deepcopy(weight_list[0])\n",
    "    # first multiply the weight of the 0th client by its proportion\n",
    "    scale0 = sizes[0] / total\n",
    "    for k in avg.keys():\n",
    "        avg[k] = avg[k] * scale0\n",
    "    # weighted accumulation of other clients\n",
    "    for i in range(1, len(weight_list)):\n",
    "        wi = weight_list[i]\n",
    "        si = sizes[i] / total\n",
    "        for k in avg.keys():\n",
    "            avg[k] += wi[k] * si\n",
    "    return avg # Returns the averaged model weights.\n",
    "\n",
    "\n",
    "def federated_training(num_clients=4,rounds=10,local_epochs=1,\n",
    "    device=\"cpu\",#mps for apple Metal series gpu, switch to cuda or cpu if otherwise\n",
    "    q=1.0, # participation rate (0, 1]\n",
    "    num_classes=100,    \n",
    "    batch_size=64,\n",
    "    lr=0.01,\n",
    "    hetero_profiles=None,   # per-client overrides (basic heterogeneity)\n",
    "):\n",
    "    global_model = models.resnet18(num_classes=num_classes)\n",
    "\n",
    "    for r in range(rounds):\n",
    "        # sample m = q * num_clients participants this round\n",
    "        m = max(1, int(q * num_clients))\n",
    "        selected = random.sample(range(num_clients), k=m)\n",
    "\n",
    "        client_states, client_sizes = [], []\n",
    "\n",
    "        for c in selected:\n",
    "            # default local hyperparams\n",
    "            ep = local_epochs\n",
    "            bs = batch_size\n",
    "            lrn = lr\n",
    "            drop_p = 0.0\n",
    "            # per-client overrides if provided\n",
    "            if hetero_profiles is not None and c in hetero_profiles:\n",
    "                prof = hetero_profiles[c]\n",
    "                ep   = prof.get(\"epochs\", ep)\n",
    "                bs   = prof.get(\"batch_size\", bs)\n",
    "                lrn  = prof.get(\"lr\", lrn)\n",
    "                drop_p = prof.get(\"dropout_prob\", 0.0)\n",
    "\n",
    "            # simulate availability (dropout)\n",
    "            if drop_p > 0 and random.random() < drop_p:\n",
    "                continue  # this client skips this round\n",
    "\n",
    "            client_dataset = Subset(train, parts[c])\n",
    "            local_state = local_train(\n",
    "                global_model, client_dataset, device,\n",
    "                epochs=ep, batch_size=bs, lr=lrn\n",
    "            )\n",
    "            client_states.append(local_state)\n",
    "            client_sizes.append(len(parts[c]))\n",
    "\n",
    "        if len(client_states) == 0:\n",
    "            print(f\"Round {r+1}: no clients participated — skip aggregation\")\n",
    "            continue\n",
    "\n",
    "        averaged = average_weights(client_states, client_sizes)\n",
    "        global_model.load_state_dict(averaged)\n",
    "        print(f\"Round {r+1}/{rounds} finished. participants={len(client_states)}/{num_clients}\")\n",
    "\n",
    "    # evaluate global model against global test set\n",
    "    global_model.to(device)\n",
    "    global_model.eval()\n",
    "    correct, total = 0, 0\n",
    "    test_loader = DataLoader(test, batch_size=128, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = global_model(x)\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    print(\"Global test acc:\", correct / total)\n",
    "    return global_model\n",
    "    \n",
    "federated_training(num_clients=number_of_clients, rounds=number_of_rounds)\n",
    "\n",
    "# e.g., make three clients slower / smaller batch / lower lr, and one flaky client\n",
    "hetero = {\n",
    "    0: {\"epochs\": 1, \"batch_size\": 32, \"lr\": 0.005, \"dropout_prob\": 0.10},\n",
    "    1: {\"epochs\": 1, \"batch_size\": 32, \"lr\": 0.005},\n",
    "    2: {\"epochs\": 1, \"batch_size\": 16, \"lr\": 0.005},\n",
    "    3: {\"dropout_prob\": 0.30},  # may skip rounds randomly\n",
    "}\n",
    "\n",
    "global_model = federated_training(\n",
    "    num_clients=number_of_clients,\n",
    "    rounds=number_of_rounds,\n",
    "    device=\"cpu\",\n",
    "    q=0.5,\n",
    "    num_classes=100,\n",
    "    hetero_profiles=hetero\n",
    ")\n",
    "print(global_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl_privacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
