{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "426f71b6-ed5c-47f9-9a35-f84920d83a5e",
   "metadata": {},
   "source": [
    "# Federated Averaging (FedAvg) baseline on CIFAR-100\n",
    "- Dirichlet non-IID partitioning\n",
    "- Partial client participation\n",
    "- Optional heterogeneity (per-client batch size / epochs / lr)\n",
    "## Imports and Config values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "129463b1-6d84-4576-b0db-1130acf5bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict, OrderedDict\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Callable\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74979dd3-9def-4756-a532-dfe7998a51cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # Federated Learning\n",
    "    \"num_rounds\": 1,\n",
    "    \"num_clients\": 20,\n",
    "    \"clients_per_round\": 5,  # Partial participation\n",
    "    \"local_epochs\": 5,\n",
    "    \"local_batch_size\": 32,\n",
    "\n",
    "    # Data\n",
    "    \"dataset\": \"CIFAR100\",\n",
    "    \"data_root\": \"./data\",\n",
    "    \"alpha\": 0.1,  # Dirichlet concentration (lower = more non-IID)\n",
    "    \"num_classes\": 100,  # 100 for CIFAR-100, 10 for CIFAR-10 etc..\n",
    "\n",
    "    # Model & Training\n",
    "    \"model_arch\": \"resnet18\",\n",
    "    \"optimizer\": \"SGD\",\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 5e-4,\n",
    "\n",
    "    # Capture settings\n",
    "    \"max_steps_to_store\": None,  # None = store all, or set limit (e.g., 50)\n",
    "    \"return_indices\": False,\n",
    "\n",
    "    # Misc\n",
    "    \"device\": \"cuda\",\n",
    "    \"seed\": 42,\n",
    "\n",
    "    # Persistence\n",
    "    \"artifact_root\": \"./reports\",\n",
    "    \"experiment_name\": \"fedavg_baseline\",\n",
    "    \"save_prefix\": \"fedavg_metrics\",\n",
    "    \"persist_client_payloads\": True,\n",
    "    \"persist_round_metrics\": True,\n",
    "    \"persist_config_snapshot\": True,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09bcf5a5-460e-4466-a956-92218dd09def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "random.seed(CONFIG[\"seed\"])\n",
    "np.random.seed(CONFIG[\"seed\"])\n",
    "torch.manual_seed(CONFIG[\"seed\"])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(CONFIG[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c3883ac-cae3-4768-aef6-cedd85c8370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def to_cpu_f32(t):\n",
    "    return t.detach().to(\"cpu\", non_blocking=True).float().clone()\n",
    "\n",
    "def state_to_cpu_f32(sd: dict):\n",
    "    return {k: to_cpu_f32(v) for k, v in sd.items()}\n",
    "\n",
    "def param_order_and_shapes(model: torch.nn.Module):\n",
    "    return [{\"name\": n, \"shape\": list(p.shape), \"numel\": p.numel()} \n",
    "            for n, p in model.named_parameters()]\n",
    "\n",
    "@dataclass\n",
    "class OptimCfg:\n",
    "    name: str = \"SGD\"\n",
    "    lr: float = 0.01\n",
    "    momentum: float = 0.9\n",
    "    weight_decay: float = 5e-4\n",
    "    nesterov: bool = False\n",
    "\n",
    "def build_optimizer(model, cfg: OptimCfg):\n",
    "    if cfg.name.lower() == \"sgd\":\n",
    "        return optim.SGD(model.parameters(), lr=cfg.lr, momentum=cfg.momentum,\n",
    "                         weight_decay=cfg.weight_decay, nesterov=cfg.nesterov)\n",
    "    elif cfg.name.lower() == \"adam\":\n",
    "        return optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {cfg.name}\")\n",
    "\n",
    "def class_histogram_from_loader(loader, num_classes: int):\n",
    "    counts = torch.zeros(num_classes, dtype=torch.long)\n",
    "    for batch in loader:\n",
    "        y = batch[1]\n",
    "        counts.index_add_(0, y.to(dtype=torch.long), torch.ones_like(y, dtype=torch.long))\n",
    "    return {int(i): int(v) for i, v in enumerate(counts)}\n",
    "\n",
    "def ensure_dir(path: Path):\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def seed_worker(worker_id: int):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "def flatten_state_dict(state: dict):\n",
    "    parts = []\n",
    "    keys = []\n",
    "    shapes = []\n",
    "    dtypes = []\n",
    "    for k, v in state.items():\n",
    "        tensor = v.detach().cpu()\n",
    "        parts.append(tensor.reshape(-1).float())\n",
    "        keys.append(k)\n",
    "        shapes.append(list(tensor.shape))\n",
    "        dtypes.append(str(tensor.dtype))\n",
    "    flat = torch.cat(parts, dim=0) if parts else torch.tensor([], dtype=torch.float32)\n",
    "    template = {\"keys\": keys, \"shapes\": shapes, \"dtypes\": dtypes}\n",
    "    return flat, template\n",
    "\n",
    "def dict_tensor_nbytes(tensor_map: dict) -> int:\n",
    "    total = 0\n",
    "    for value in tensor_map.values():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            total += value.element_size() * value.numel()\n",
    "    return int(total)\n",
    "\n",
    "def dict_tensor_norm(tensor_map: dict) -> float:\n",
    "    total = 0.0\n",
    "    for value in tensor_map.values():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            total += float(value.float().pow(2).sum().item())\n",
    "    return float(math.sqrt(total)) if total > 0.0 else 0.0\n",
    "\n",
    "def json_default(obj):\n",
    "    if isinstance(obj, Path):\n",
    "        return str(obj)\n",
    "    if isinstance(obj, torch.device):\n",
    "        return str(obj)\n",
    "    if isinstance(obj, np.generic):\n",
    "        return obj.item()\n",
    "    raise TypeError(f\"Object of type {type(obj).__name__} is not JSON serializable\")\n",
    "\n",
    "def save_client_update(\n",
    "    experiment_dir: Path,\n",
    "    round_idx: int,\n",
    "    client_id: int,\n",
    "    global_state: dict,\n",
    "    local_state: dict,\n",
    "    shard_size: int,\n",
    "    lr: float,\n",
    "    epochs: int,\n",
    "    extra_meta: Optional[dict] = None,\n",
    ") -> Path:\n",
    "    round_dir = Path(experiment_dir) / f\"round_{int(round_idx):02d}\"\n",
    "    ensure_dir(round_dir)\n",
    "\n",
    "    local_flat, template = flatten_state_dict(local_state)\n",
    "    global_flat, _ = flatten_state_dict(global_state)\n",
    "    delta = (local_flat - global_flat).cpu()\n",
    "\n",
    "    meta = {\n",
    "        \"round\": int(round_idx),\n",
    "        \"client_id\": int(client_id),\n",
    "        \"shard_size\": int(shard_size),\n",
    "        \"lr\": float(lr),\n",
    "        \"epochs\": int(epochs),\n",
    "    }\n",
    "    if extra_meta:\n",
    "        meta.update(extra_meta)\n",
    "\n",
    "    payload = {\n",
    "        \"delta\": delta,\n",
    "        \"template\": template,\n",
    "        \"meta\": meta,\n",
    "    }\n",
    "\n",
    "    path = round_dir / f\"client_{client_id}.pt\"\n",
    "    torch.save(payload, path)\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640693cd-8558-4039-94a6-73e1c7310bb8",
   "metadata": {},
   "source": [
    "### Data: CIFAR-100 loaders (train/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "339ab6c7-e890-45c8-a328-58abc50f6b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar100(data_root: str = \"./data\"):\n",
    "    mean = (0.5071, 0.4867, 0.4408)\n",
    "    std = (0.2675, 0.2565, 0.2761)\n",
    "    \n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "    test_tf = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "    \n",
    "    train = datasets.CIFAR100(root=data_root, train=True, download=True, transform=train_tf)\n",
    "    test = datasets.CIFAR100(root=data_root, train=False, download=True, transform=test_tf)\n",
    "    return train, test\n",
    "\n",
    "def _get_targets(dataset) -> np.ndarray:\n",
    "    targets = getattr(dataset, \"targets\", None)\n",
    "    if targets is None:\n",
    "        targets = getattr(dataset, \"labels\", None)\n",
    "    if targets is None:\n",
    "        raise AttributeError(\"Dataset has no 'targets' or 'labels'.\")\n",
    "    return np.array(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181cb369-91c8-4418-9c36-7c8d1066c6ad",
   "metadata": {},
   "source": [
    "Dirichlet non-IID split (returns dict: client_id -> list of indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ffb5136-9ee7-4c9e-91b3-93cb31543de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dirichlet_noniid_indices(dataset, num_clients: int, alpha: float, \n",
    "                             min_per_client: int = 10) -> Dict[int, List[int]]:\n",
    "    y = _get_targets(dataset)\n",
    "    num_classes = int(y.max()) + 1\n",
    "    idx_by_class = {c: np.where(y == c)[0] for c in range(num_classes)}\n",
    "    for c in idx_by_class:\n",
    "        np.random.shuffle(idx_by_class[c])\n",
    "    \n",
    "    client_indices = [[] for _ in range(num_clients)]\n",
    "    for c in range(num_classes):\n",
    "        idx_c = idx_by_class[c]\n",
    "        if len(idx_c) == 0:\n",
    "            continue\n",
    "        p = np.random.dirichlet([alpha] * num_clients)\n",
    "        cuts = (np.cumsum(p) * len(idx_c)).astype(int)[:-1]\n",
    "        split = np.split(idx_c, cuts)\n",
    "        for i, shard in enumerate(split):\n",
    "            client_indices[i].extend(shard.tolist())\n",
    "    \n",
    "    pool = list(range(len(dataset)))\n",
    "    for i in range(num_clients):\n",
    "        if len(client_indices[i]) < min_per_client:\n",
    "            need = min_per_client - len(client_indices[i])\n",
    "            extra = np.random.choice(pool, size=need, replace=False).tolist()\n",
    "            client_indices[i].extend(extra)\n",
    "    \n",
    "    for i in range(num_clients):\n",
    "        random.shuffle(client_indices[i])\n",
    "    return {i: client_indices[i] for i in range(num_clients)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1ab77a-7051-4c8a-a378-17572b213790",
   "metadata": {},
   "source": [
    "### Model: ResNet18 head for CIFAR-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed2e60c4-fe88-4517-82d1-6a378cec15bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes: int = 100) -> nn.Module:\n",
    "    model = models.resnet18(weights=None)  # no pretrained to avoid download in restricted envs\n",
    "    # CIFAR images are 3x32x32; torchvision ResNet expects 224x224,\n",
    "    # but it's fine—ResNet is fully conv except FC. It still works on 32x32.\n",
    "    # Replace final FC layer to match number of classes\n",
    "    in_feats = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_feats, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4147e8a-70fb-4f6d-8ef4-47f142fbeea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, loader: DataLoader, device: torch.device) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_sum = 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss_sum += loss.item() * x.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += x.size(0)\n",
    "    return loss_sum / max(1, total), correct / max(1, total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2a53dc-5017-4571-801b-cca18e22badd",
   "metadata": {},
   "source": [
    "###  Local Training with Gradient Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51d75124-15ce-4c2b-9faa-7a66984edd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def clone_state(model):\n",
    "    return {k: v.detach().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "def train_one_client_with_capture(\n",
    "    global_model: nn.Module,\n",
    "    client_loader: DataLoader,\n",
    "    loss_fn: nn.Module,\n",
    "    opt_cfg: OptimCfg,\n",
    "    epochs: int = 1,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    "    max_steps_to_store: int = None,\n",
    "    return_indices: bool = False,\n",
    "    num_classes: int = None,\n",
    "    client_seed: int = None\n",
    "):\n",
    "    model = copy.deepcopy(global_model).to(device)\n",
    "    model.train()\n",
    "    \n",
    "    if client_seed is not None:\n",
    "        torch.manual_seed(client_seed)\n",
    "        random.seed(client_seed)\n",
    "        np.random.seed(client_seed)\n",
    "    \n",
    "    global_before = clone_state(model)\n",
    "    opt = build_optimizer(model, opt_cfg)\n",
    "    \n",
    "    grads_per_step_raw = []\n",
    "    grads_per_step_wd = []\n",
    "    batch_sizes = []\n",
    "    step_losses = []\n",
    "    step_batch_indices = []\n",
    "    \n",
    "    steps_stored = 0\n",
    "    for _ in range(epochs):\n",
    "        for batch in client_loader:\n",
    "            if return_indices and len(batch) == 3:\n",
    "                x, y, idxs = batch\n",
    "            else:\n",
    "                x, y = batch[0], batch[1]\n",
    "                idxs = None\n",
    "            \n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            \n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(x)\n",
    "            loss = loss_fn(logits, y)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Capture gradients before optimizer step\n",
    "            if max_steps_to_store is None or steps_stored < max_steps_to_store:\n",
    "                raw_dict = {}\n",
    "                wd_dict = {}\n",
    "                for name, p in model.named_parameters():\n",
    "                    if p.grad is None:\n",
    "                        continue\n",
    "                    g = p.grad\n",
    "                    raw_dict[name] = to_cpu_f32(g)\n",
    "                    if opt_cfg.weight_decay and opt_cfg.weight_decay > 0:\n",
    "                        wd_dict[name] = to_cpu_f32(g + opt_cfg.weight_decay * p.data)\n",
    "                    else:\n",
    "                        wd_dict[name] = to_cpu_f32(g)\n",
    "                \n",
    "                grads_per_step_raw.append(raw_dict)\n",
    "                grads_per_step_wd.append(wd_dict)\n",
    "                batch_sizes.append(int(x.shape[0]))\n",
    "                step_losses.append(float(loss.detach().item()))\n",
    "                if return_indices and idxs is not None:\n",
    "                    step_batch_indices.append([int(i) for i in idxs])\n",
    "                \n",
    "                steps_stored += 1\n",
    "            \n",
    "            opt.step()\n",
    "    \n",
    "    local_after = clone_state(model)\n",
    "    \n",
    "    # Compute delta\n",
    "    delta = OrderedDict()\n",
    "    for k in local_after.keys():\n",
    "        delta[k] = to_cpu_f32(local_after[k]) - to_cpu_f32(global_before[k])\n",
    "    \n",
    "    # Diagnostics\n",
    "    if len(grads_per_step_raw) > 0:\n",
    "        first_step = grads_per_step_raw[0]\n",
    "        per_layer_norms = {k: float(v.view(-1).norm().item()) for k, v in first_step.items()}\n",
    "        grad_norm_total = float(torch.sqrt(sum(v.pow(2).sum() for v in first_step.values())).item())\n",
    "    else:\n",
    "        per_layer_norms, grad_norm_total = {}, 0.0\n",
    "    \n",
    "    class_dist = None\n",
    "    if num_classes is not None:\n",
    "        class_dist = class_histogram_from_loader(client_loader, num_classes=num_classes)\n",
    "    \n",
    "    telemetry = {\n",
    "        \"per_layer_norms\": per_layer_norms,\n",
    "        \"gradient_norm\": grad_norm_total,\n",
    "        \"loss_history\": step_losses,\n",
    "        \"batch_sizes\": batch_sizes,\n",
    "        \"num_steps_captured\": len(grads_per_step_raw),\n",
    "        \"num_samples\": sum(batch_sizes),\n",
    "        \"class_distribution\": class_dist,\n",
    "    }\n",
    "    if return_indices and len(step_batch_indices) > 0:\n",
    "        telemetry[\"batch_indices\"] = step_batch_indices\n",
    "    \n",
    "    return {\n",
    "        \"local_state_after\": state_to_cpu_f32(local_after),\n",
    "        \"delta\": delta,\n",
    "        \"grads_per_step_raw\": grads_per_step_raw,\n",
    "        \"grads_per_step_wd\": grads_per_step_wd,\n",
    "        \"telemetry\": telemetry,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b2afc1-e8b4-4d9f-ba4f-0ade607afdfb",
   "metadata": {},
   "source": [
    "### FedAvg Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b7ff223-24df-4e1d-88e1-66f41d5fa6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_weights(weight_list, sizes):\n",
    "    if not weight_list:\n",
    "        raise ValueError(\"No client weights provided.\")\n",
    "    if len(weight_list) != len(sizes):\n",
    "        raise ValueError(\"weights and sizes mismatch\")\n",
    "    \n",
    "    total = float(sum(sizes))\n",
    "    avg = {k: torch.zeros_like(v) for k, v in weight_list[0].items()}\n",
    "    \n",
    "    for wi, si in zip(weight_list, sizes):\n",
    "        w = si / total\n",
    "        for k in avg.keys():\n",
    "            if avg[k].dtype.is_floating_point:\n",
    "                avg[k] += wi[k].float() * w\n",
    "            else:\n",
    "                avg[k] = wi[k].clone()\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c37ebcf-4a6f-43b0-b8de-33a93110910a",
   "metadata": {},
   "source": [
    "## Federated Round with Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef678f95-631a-4348-b079-afee529f6c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fed_round_with_capture(\n",
    "    round_num: int,\n",
    "    global_model: nn.Module,\n",
    "    clients: dict,\n",
    "    loss_fn: nn.Module,\n",
    "    opt_cfg: OptimCfg,\n",
    "    local_epochs: int,\n",
    "    device: torch.device,\n",
    "    num_classes: int = None,\n",
    "    max_steps_to_store: int = None,\n",
    "    return_indices: bool = False,\n",
    "    server_seed: int = None,\n",
    "    client_seeds: dict = None,\n",
    "    training_meta: dict = None,\n",
    "    global_eval_fn = None,\n",
    "    client_callback: Optional[Callable[[int, int, dict, dict], None]] = None,\n",
    "):\n",
    "    if server_seed is not None:\n",
    "        torch.manual_seed(server_seed)\n",
    "        random.seed(server_seed)\n",
    "        np.random.seed(server_seed)\n",
    "\n",
    "    participating_clients = list(clients.keys())\n",
    "    global_state_cpu = state_to_cpu_f32(global_model.state_dict())\n",
    "\n",
    "    client_metrics = {}\n",
    "    raw_gradients = {}\n",
    "    model_updates = {}\n",
    "\n",
    "    for cid, loader in clients.items():\n",
    "        cseed = (client_seeds or {}).get(cid)\n",
    "        result = train_one_client_with_capture(\n",
    "            global_model=global_model,\n",
    "            client_loader=loader,\n",
    "            loss_fn=loss_fn,\n",
    "            opt_cfg=opt_cfg,\n",
    "            epochs=local_epochs,\n",
    "            device=device,\n",
    "            max_steps_to_store=max_steps_to_store,\n",
    "            return_indices=return_indices,\n",
    "            num_classes=num_classes,\n",
    "            client_seed=cseed,\n",
    "        )\n",
    "\n",
    "        model_updates[cid] = result[\"delta\"]\n",
    "        raw_gradients[cid] = {\n",
    "            \"grads_per_step_raw\": result[\"grads_per_step_raw\"],\n",
    "            \"grads_per_step_wd\": result[\"grads_per_step_wd\"],\n",
    "        }\n",
    "\n",
    "        delta_bytes = dict_tensor_nbytes(result[\"delta\"])\n",
    "        delta_norm = dict_tensor_norm(result[\"delta\"])\n",
    "\n",
    "        tele = result[\"telemetry\"]\n",
    "        client_metrics[cid] = {\n",
    "            \"gradient_norm\": tele[\"gradient_norm\"],\n",
    "            \"per_layer_norms\": tele[\"per_layer_norms\"],\n",
    "            \"local_epochs\": local_epochs,\n",
    "            \"learning_rate\": opt_cfg.lr,\n",
    "            \"num_samples\": tele[\"num_samples\"],\n",
    "            \"class_distribution\": tele[\"class_distribution\"],\n",
    "            \"local_loss\": float(tele[\"loss_history\"][-1]) if tele[\"loss_history\"] else None,\n",
    "            \"loss_history\": tele[\"loss_history\"],\n",
    "            \"batch_sizes\": tele[\"batch_sizes\"],\n",
    "            \"num_steps_captured\": tele.get(\"num_steps_captured\"),\n",
    "            \"update_norm\": delta_norm,\n",
    "            \"upload_bytes\": delta_bytes,\n",
    "            \"seed\": cseed,\n",
    "        }\n",
    "        if return_indices and (\"batch_indices\" in tele):\n",
    "            client_metrics[cid][\"batch_indices\"] = tele[\"batch_indices\"]\n",
    "\n",
    "        if callable(client_callback):\n",
    "            client_callback(round_num, cid, result, global_state_cpu)\n",
    "\n",
    "    agg_delta = {}\n",
    "    if len(model_updates) > 0:\n",
    "        keys = next(iter(model_updates.values())).keys()\n",
    "        sizes = [client_metrics[cid][\"num_samples\"] for cid in participating_clients]\n",
    "        total_samples = float(sum(sizes)) if sizes else 0.0\n",
    "        for k in keys:\n",
    "            agg = torch.zeros_like(model_updates[participating_clients[0]][k])\n",
    "            for cid, size in zip(participating_clients, sizes):\n",
    "                weight = float(size) / total_samples if total_samples > 0 else 0.0\n",
    "                agg = agg + model_updates[cid][k] * weight\n",
    "            agg_delta[k] = agg\n",
    "\n",
    "    global_accuracy = None\n",
    "    global_loss = None\n",
    "    if callable(global_eval_fn):\n",
    "        global_loss, global_accuracy = global_eval_fn(global_model)\n",
    "\n",
    "    config_snapshot = {\n",
    "        \"arch\": type(global_model).__name__,\n",
    "        \"optimizer\": asdict(opt_cfg),\n",
    "        \"loss\": type(loss_fn).__name__,\n",
    "        \"num_classes\": num_classes,\n",
    "        \"param_meta\": param_order_and_shapes(global_model),\n",
    "        \"seeds\": {\"server_seed\": server_seed, \"client_seeds\": client_seeds},\n",
    "        \"device\": str(device),\n",
    "    }\n",
    "    if training_meta:\n",
    "        config_snapshot.update({\"training_meta\": training_meta})\n",
    "\n",
    "    update_norms = {int(cid): client_metrics[cid][\"update_norm\"] for cid in participating_clients}\n",
    "    norm_values = np.array(list(update_norms.values()), dtype=float) if update_norms else np.array([], dtype=float)\n",
    "    round_stats = {\n",
    "        \"update_norms\": update_norms,\n",
    "        \"server_delta_norm\": dict_tensor_norm(agg_delta),\n",
    "        \"avg_update_norm\": float(norm_values.mean()) if norm_values.size else 0.0,\n",
    "        \"std_update_norm\": float(norm_values.std()) if norm_values.size > 1 else 0.0,\n",
    "        \"max_update_norm\": float(norm_values.max()) if norm_values.size else 0.0,\n",
    "        \"min_update_norm\": float(norm_values.min()) if norm_values.size else 0.0,\n",
    "        \"num_participating_clients\": len(participating_clients),\n",
    "    }\n",
    "\n",
    "    client_upload_details = {int(cid): int(client_metrics[cid][\"upload_bytes\"]) for cid in participating_clients}\n",
    "    communication_bytes = {\n",
    "        \"server_broadcast\": dict_tensor_nbytes(global_state_cpu),\n",
    "        \"client_upload_total\": int(sum(client_upload_details.values())),\n",
    "        \"client_upload_per_client\": client_upload_details,\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"round\": int(round_num),\n",
    "        \"participating_clients\": participating_clients,\n",
    "        \"client_metrics\": client_metrics,\n",
    "        \"global_model_state\": global_state_cpu,\n",
    "        \"global_accuracy\": global_accuracy,\n",
    "        \"global_loss\": global_loss,\n",
    "        \"raw_gradients\": raw_gradients,\n",
    "        \"model_updates\": model_updates,\n",
    "        \"server_aggregate_delta\": agg_delta,\n",
    "        \"config_snapshot\": config_snapshot,\n",
    "        \"round_stats\": round_stats,\n",
    "        \"communication_bytes\": communication_bytes,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df6900b-5545-4c70-beb3-a847992afc01",
   "metadata": {},
   "source": [
    "## Save Exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62121657-e9f6-400b-9980-f3b76d35b393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "def save_round_export(\n",
    "    metrics_to_export,\n",
    "    experiment_dir: Path,\n",
    "    prefix: str = \"fed_round\",\n",
    "    persist_json: bool = True,\n",
    "):\n",
    "    r = metrics_to_export[\"round\"]\n",
    "    ensure_dir(experiment_dir)\n",
    "    round_dir = Path(experiment_dir) / f\"round_{r:02d}\"\n",
    "    ensure_dir(round_dir)\n",
    "\n",
    "    tensor_blob = {\n",
    "        \"global_model_state\": metrics_to_export[\"global_model_state\"],\n",
    "        \"raw_gradients\": metrics_to_export[\"raw_gradients\"],\n",
    "        \"model_updates\": metrics_to_export[\"model_updates\"],\n",
    "        \"server_aggregate_delta\": metrics_to_export.get(\"server_aggregate_delta\"),\n",
    "    }\n",
    "    meta_blob = {\n",
    "        k: v\n",
    "        for k, v in metrics_to_export.items()\n",
    "        if k not in tensor_blob.keys()\n",
    "    }\n",
    "\n",
    "    tensor_path = round_dir / f\"{prefix}_{r:02d}_tensors.pt\"\n",
    "    torch.save(tensor_blob, tensor_path)\n",
    "\n",
    "    meta_path = None\n",
    "    if persist_json:\n",
    "        meta_path = round_dir / f\"{prefix}_{r:02d}_meta.json\"\n",
    "        with open(meta_path, \"w\") as f:\n",
    "            json.dump(meta_blob, f, indent=2, default=json_default)\n",
    "    else:\n",
    "        meta_path = round_dir / f\"{prefix}_{r:02d}_meta.pkl\"\n",
    "        with open(meta_path, \"wb\") as f:\n",
    "            pickle.dump(meta_blob, f)\n",
    "\n",
    "    return {\n",
    "        \"round_dir\": str(round_dir),\n",
    "        \"tensor_path\": str(tensor_path),\n",
    "        \"meta_path\": str(meta_path) if meta_path else None,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb72276a-9d1b-47fe-8d8d-014832f70ea4",
   "metadata": {},
   "source": [
    "## Final / main executionn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb95684-ccc1-4760-97e0-248f8687d1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"FedAvg Training with Gradient Capture\")\n",
    "    print(f\"Device preference: {CONFIG['device']}\")\n",
    "    \n",
    "    device = torch.device(CONFIG[\"device\"] if torch.cuda.is_available() else \"mps\")\n",
    "\n",
    "    print(f\"Resolved device: {device}\")\n",
    "    \n",
    "    print(f\"Rounds: {CONFIG['num_rounds']}\")\n",
    "    print(f\"Clients: {CONFIG['num_clients']} (sampling {CONFIG['clients_per_round']}/round)\")\n",
    "    print(f\"Local epochs: {CONFIG['local_epochs']}\")\n",
    "    print(f\"Alpha (non-IID): {CONFIG['alpha']}\")\n",
    "    print(\"_\" * 100)\n",
    "\n",
    "    pin_memory = (device.type == \"cuda\")\n",
    "\n",
    "    # Load data\n",
    "    print(\"\\n[1/5] Loading data...\")\n",
    "    train_dataset, test_dataset = load_cifar100(CONFIG[\"data_root\"])\n",
    "    \n",
    "    # Set num_workers=0 to avoid multiprocessing fork issues\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=256, \n",
    "        shuffle=False,  # No need to shuffle test data\n",
    "        num_workers=0,  # Avoid fork issues\n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "\n",
    "    # Partition data\n",
    "    print(\"[2/5] Partitioning data (Dirichlet non-IID)...\")\n",
    "    client_indices = dirichlet_noniid_indices(\n",
    "        train_dataset,\n",
    "        CONFIG[\"num_clients\"],\n",
    "        CONFIG[\"alpha\"],\n",
    "    )\n",
    "\n",
    "    client_loaders = {}\n",
    "    for cid, indices in client_indices.items():\n",
    "        subset = Subset(train_dataset, indices)\n",
    "        generator = torch.Generator()\n",
    "        generator.manual_seed(CONFIG[\"seed\"] + int(cid))\n",
    "        \n",
    "        # Fix: Set num_workers=0 to avoid fork-based multiprocessing issues\n",
    "        client_loaders[cid] = DataLoader(\n",
    "            subset,\n",
    "            batch_size=CONFIG[\"local_batch_size\"],\n",
    "            shuffle=True,\n",
    "            num_workers=0,  # Critical fix for memory allocation error\n",
    "            worker_init_fn=seed_worker,\n",
    "            generator=generator,\n",
    "            pin_memory=pin_memory,\n",
    "        )\n",
    "\n",
    "    print(f\"   Client data sizes: {[len(idx) for idx in client_indices.values()]}\")\n",
    "\n",
    "    # Initialize model\n",
    "    print(\"[3/5] Building model...\")\n",
    "    global_model = build_model(CONFIG[\"num_classes\"]).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    opt_cfg = OptimCfg(\n",
    "        name=CONFIG[\"optimizer\"],\n",
    "        lr=CONFIG[\"learning_rate\"],\n",
    "        momentum=CONFIG[\"momentum\"],\n",
    "        weight_decay=CONFIG[\"weight_decay\"],\n",
    "    )\n",
    "\n",
    "    experiment_dir = Path(CONFIG[\"artifact_root\"]) / CONFIG[\"experiment_name\"]\n",
    "    ensure_dir(experiment_dir)\n",
    "    \n",
    "    if CONFIG.get(\"persist_config_snapshot\", True):\n",
    "        config_snapshot = dict(CONFIG)\n",
    "        config_snapshot.update(\n",
    "            {\n",
    "                \"resolved_device\": str(device),\n",
    "                \"param_meta\": param_order_and_shapes(global_model),\n",
    "            }\n",
    "        )\n",
    "        with open(experiment_dir / \"experiment_config.json\", \"w\") as f:\n",
    "            json.dump(config_snapshot, f, indent=2, default=json_default)\n",
    "\n",
    "    # Training loop\n",
    "    print(\"[4/5] Starting federated training...\")\n",
    "    for round_num in range(CONFIG[\"num_rounds\"]):\n",
    "        print(f\"\\n--- Round {round_num + 1}/{CONFIG['num_rounds']} ---\")\n",
    "\n",
    "        participating = random.sample(\n",
    "            list(client_loaders.keys()),\n",
    "            CONFIG[\"clients_per_round\"],\n",
    "        )\n",
    "        selected_loaders = {cid: client_loaders[cid] for cid in participating}\n",
    "\n",
    "        round_start = time.perf_counter()\n",
    "        client_artifact_paths = {}\n",
    "\n",
    "        client_callback = None\n",
    "        if CONFIG.get(\"persist_client_payloads\", True):\n",
    "            def _callback(r_idx, client_id, result, global_state):\n",
    "                tele = result[\"telemetry\"]\n",
    "                seed_value = None\n",
    "                if client_seeds and client_id in client_seeds:\n",
    "                    seed_value = client_seeds[client_id]\n",
    "                elif CONFIG.get(\"seed\") is not None:\n",
    "                    seed_value = CONFIG[\"seed\"] + int(client_id)\n",
    "                \n",
    "                extra_meta = {\n",
    "                    \"seed\": seed_value,\n",
    "                    \"num_samples\": int(tele[\"num_samples\"]),\n",
    "                    \"num_batches\": len(tele[\"batch_sizes\"]),\n",
    "                    \"batch_sizes\": [int(b) for b in tele[\"batch_sizes\"]],\n",
    "                    \"gradient_norm\": tele[\"gradient_norm\"],\n",
    "                    \"num_steps_captured\": tele.get(\"num_steps_captured\"),\n",
    "                }\n",
    "                if tele.get(\"class_distribution\") is not None:\n",
    "                    extra_meta[\"class_distribution\"] = tele[\"class_distribution\"]\n",
    "                if tele.get(\"loss_history\"):\n",
    "                    extra_meta[\"loss_history\"] = tele[\"loss_history\"]\n",
    "                \n",
    "                path = save_client_update(\n",
    "                    experiment_dir=experiment_dir,\n",
    "                    round_idx=r_idx,\n",
    "                    client_id=client_id,\n",
    "                    global_state=global_state,\n",
    "                    local_state=result[\"local_state_after\"],\n",
    "                    shard_size=tele[\"num_samples\"],\n",
    "                    lr=opt_cfg.lr,\n",
    "                    epochs=CONFIG[\"local_epochs\"],\n",
    "                    extra_meta=extra_meta,\n",
    "                )\n",
    "                client_artifact_paths[int(client_id)] = str(path.relative_to(experiment_dir))\n",
    "\n",
    "            client_callback = _callback\n",
    "\n",
    "        server_seed = CONFIG[\"seed\"] + round_num if CONFIG.get(\"seed\") is not None else None\n",
    "        client_seeds = {cid: CONFIG[\"seed\"] + round_num + int(cid) for cid in participating} if CONFIG.get(\"seed\") is not None else None\n",
    "\n",
    "        metrics = run_fed_round_with_capture(\n",
    "            round_num=round_num,\n",
    "            global_model=global_model,\n",
    "            clients=selected_loaders,\n",
    "            loss_fn=loss_fn,\n",
    "            opt_cfg=opt_cfg,\n",
    "            local_epochs=CONFIG[\"local_epochs\"],\n",
    "            device=device,  \n",
    "            num_classes=CONFIG[\"num_classes\"],\n",
    "            max_steps_to_store=CONFIG[\"max_steps_to_store\"],\n",
    "            return_indices=CONFIG[\"return_indices\"],\n",
    "            server_seed=server_seed,\n",
    "            client_seeds=client_seeds,\n",
    "            training_meta={\"dataset\": CONFIG[\"dataset\"], \"alpha\": CONFIG[\"alpha\"]},\n",
    "            global_eval_fn=lambda m: evaluate(m, test_loader, device),\n",
    "            client_callback=client_callback,\n",
    "        )\n",
    "\n",
    "        metrics[\"round_wall_time_sec\"] = time.perf_counter() - round_start\n",
    "        metrics[\"client_artifacts\"] = client_artifact_paths\n",
    "\n",
    "        if client_artifact_paths:\n",
    "            for cid, path in client_artifact_paths.items():\n",
    "                if cid in metrics[\"client_metrics\"]:\n",
    "                    metrics[\"client_metrics\"][cid][\"artifact_path\"] = path\n",
    "\n",
    "        # Update global model\n",
    "        current_state = global_model.state_dict()\n",
    "        new_state = {}\n",
    "        for k in current_state.keys():\n",
    "            if k in metrics[\"server_aggregate_delta\"]:\n",
    "                new_state[k] = current_state[k] + metrics[\"server_aggregate_delta\"][k].to(device)\n",
    "            else:\n",
    "                new_state[k] = current_state[k]\n",
    "        global_model.load_state_dict(new_state)\n",
    "\n",
    "        print(f\"   Clients: {participating}\")\n",
    "        print(f\"   Global Loss: {metrics['global_loss']:.4f}\" if metrics['global_loss'] is not None else \"   Global Loss: n/a\")\n",
    "        print(f\"   Global Acc: {metrics['global_accuracy']:.4f}\" if metrics['global_accuracy'] is not None else \"   Global Acc: n/a\")\n",
    "        upload_mb = metrics[\"communication_bytes\"][\"client_upload_total\"] / 1e6 if metrics[\"communication_bytes\"][\"client_upload_total\"] else 0.0\n",
    "        print(f\"   Round time: {metrics['round_wall_time_sec']:.2f}s | Client upload: {upload_mb:.2f} MB\")\n",
    "\n",
    "        export_paths = save_round_export(\n",
    "            metrics,\n",
    "            experiment_dir=experiment_dir,\n",
    "            prefix=CONFIG[\"save_prefix\"],\n",
    "            persist_json=CONFIG.get(\"persist_round_metrics\", True),\n",
    "        )\n",
    "\n",
    "        def _rel(path_str):\n",
    "            if path_str is None:\n",
    "                return None\n",
    "            p = Path(path_str)\n",
    "            try:\n",
    "                return str(p.relative_to(experiment_dir))\n",
    "            except ValueError:\n",
    "                return str(p)\n",
    "\n",
    "        metrics[\"round_artifacts\"] = {\n",
    "            \"round_dir\": _rel(export_paths[\"round_dir\"]),\n",
    "            \"tensor\": _rel(export_paths[\"tensor_path\"]),\n",
    "            \"meta\": _rel(export_paths.get(\"meta_path\")),\n",
    "        }\n",
    "        print(f\"   Saved artifacts under {metrics['round_artifacts']['round_dir']}\")\n",
    "\n",
    "    print(\"\\n[5/5] Final evaluation...\")\n",
    "    final_loss, final_acc = evaluate(global_model, test_loader, device)\n",
    "    print(f\"   Final Test Loss: {final_loss:.4f}\")\n",
    "    print(f\"   Final Test Accuracy: {final_acc:.4f}\")\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 100)\n",
    "    print(\"Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
