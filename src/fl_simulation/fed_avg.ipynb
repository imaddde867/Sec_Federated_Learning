{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ab3e9f-0b37-449f-be03-abf4314f2bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and simulation defaults\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "number_of_clients = 4\n",
    "number_of_rounds = 1\n",
    "dirichlet_alphas = [0.5, 0.1]\n",
    "rng_seed = 42\n",
    "\n",
    "random.seed(rng_seed)\n",
    "np.random.seed(rng_seed)\n",
    "torch.manual_seed(rng_seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084b8e46-3dc7-409f-8135-d8e27533d768",
   "metadata": {},
   "source": [
    "In healthcare federated learning each site treats a distinct patient cohort, so their data distributions diverge.\n",
    "Assuming IID shards would imply identical caseloads everywhere and can hide clinically critical edge cases.\n",
    "We therefore rely exclusively on Dirichlet-driven non-IID splits throughout this simulator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2771e9-3723-47b2-9c3f-ad7dc2596a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar100(data_root=\"./data\"):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5071, 0.4867, 0.4408),\n",
    "                         (0.2675, 0.2565, 0.2761))\n",
    "    ])\n",
    "    train = datasets.CIFAR100(root=data_root, train=True, download=True, transform=transform)\n",
    "    test = datasets.CIFAR100(root=data_root, train=False, download=True, transform=transform)\n",
    "    return train, test\n",
    "\n",
    "train_dataset, test_dataset = load_cifar100()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb01b8e-8c2e-41ad-bded-3e474f2b06e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_targets(dataset):\n",
    "    targets = getattr(dataset, \"targets\", None)\n",
    "    if targets is None:\n",
    "        targets = getattr(dataset, \"labels\", None)\n",
    "    if targets is None:\n",
    "        raise ValueError(\"Dataset does not expose targets or labels for partitioning\")\n",
    "    if torch.is_tensor(targets):\n",
    "        targets = targets.numpy()\n",
    "    return np.asarray(targets)\n",
    "\n",
    "def partition_dirichlet(dataset, num_clients, alpha=0.5, seed=None):\n",
    "    if alpha <= 0:\n",
    "        raise ValueError(\"Dirichlet alpha must be positive.\")\n",
    "    targets = _get_targets(dataset)\n",
    "    classes = np.unique(targets)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    partitions = [[] for _ in range(num_clients)]\n",
    "\n",
    "    for cls in classes:\n",
    "        cls_indices = np.where(targets == cls)[0]\n",
    "        if cls_indices.size == 0:\n",
    "            continue\n",
    "        rng.shuffle(cls_indices)\n",
    "        allocation = rng.dirichlet(np.full(num_clients, alpha))\n",
    "        expected = allocation * cls_indices.size\n",
    "        counts = expected.astype(int)\n",
    "        residue = cls_indices.size - counts.sum()\n",
    "        if residue > 0:\n",
    "            order = np.argsort(-(expected - counts))\n",
    "            for client_id in order[:residue]:\n",
    "                counts[client_id] += 1\n",
    "        start = 0\n",
    "        for client_id, take in enumerate(counts):\n",
    "            if take <= 0:\n",
    "                continue\n",
    "            end = start + take\n",
    "            partitions[client_id].extend(cls_indices[start:end].tolist())\n",
    "            start = end\n",
    "\n",
    "    for client_id in range(num_clients):\n",
    "        rng.shuffle(partitions[client_id])\n",
    "\n",
    "    return partitions\n",
    "\n",
    "dirichlet_partitions = {\n",
    "    alpha: partition_dirichlet(\n",
    "        train_dataset,\n",
    "        number_of_clients,\n",
    "        alpha=alpha,\n",
    "        seed=rng_seed + int(alpha * 1000)\n",
    "    )\n",
    "    for alpha in dirichlet_alphas\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf8848e-e4d2-412c-8af9-ad8d388c17e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_train(model, dataset, device, epochs=1, batch_size=32, lr=0.01):\n",
    "    local_model = copy.deepcopy(model)\n",
    "    local_model.to(device)\n",
    "    local_model.train()\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    opt = optim.SGD(local_model.parameters(), lr=lr, momentum=0.9)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            opt.zero_grad()\n",
    "            out = local_model(x)\n",
    "            loss = loss_fn(out, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "    return {k: v.detach().cpu() for k, v in local_model.state_dict().items()}\n",
    "\n",
    "def average_weights(weight_list, sizes):\n",
    "    if not weight_list:\n",
    "        raise ValueError(\"No client weights provided for averaging.\")\n",
    "    if len(weight_list) != len(sizes):\n",
    "        raise ValueError(\"Number of weights and sizes must match.\")\n",
    "    total = float(sum(sizes))\n",
    "    if total <= 0:\n",
    "        raise ValueError(\"Total client sample size must be positive.\")\n",
    "\n",
    "    avg = copy.deepcopy(weight_list[0])\n",
    "    scale0 = sizes[0] / total\n",
    "    for k in avg.keys():\n",
    "        avg[k] = avg[k] * scale0\n",
    "    for idx in range(1, len(weight_list)):\n",
    "        wi = weight_list[idx]\n",
    "        si = sizes[idx] / total\n",
    "        for k in avg.keys():\n",
    "            avg[k] += wi[k] * si\n",
    "    return avg\n",
    "\n",
    "def federated_training(\n",
    "    train_dataset,\n",
    "    test_dataset,\n",
    "    partitions,\n",
    "    rounds=10,\n",
    "    local_epochs=1,\n",
    "    device=\"mps\",\n",
    "    q=1.0,\n",
    "    num_classes=100,\n",
    "    batch_size=64,\n",
    "    lr=0.01,\n",
    "    hetero_profiles=None,\n",
    "):\n",
    "    num_clients = len(partitions)\n",
    "    if num_clients == 0:\n",
    "        raise ValueError(\"No clients available for federated training.\")\n",
    "    if not (0 < q <= 1):\n",
    "        raise ValueError(\"Client participation rate q must be in (0, 1].\")\n",
    "\n",
    "    global_model = models.resnet18(num_classes=num_classes)\n",
    "\n",
    "    for r in range(rounds):\n",
    "        m = max(1, min(num_clients, int(q * num_clients)))\n",
    "        selected = random.sample(range(num_clients), k=m)\n",
    "        client_states = []\n",
    "        client_sizes = []\n",
    "\n",
    "        for client_id in selected:\n",
    "            shard = partitions[client_id]\n",
    "            if not shard:\n",
    "                continue\n",
    "\n",
    "            epochs = local_epochs\n",
    "            batch = batch_size\n",
    "            client_lr = lr\n",
    "            dropout_prob = 0.0\n",
    "            if hetero_profiles:\n",
    "                profile = hetero_profiles.get(client_id, {})\n",
    "                epochs = profile.get(\"epochs\", epochs)\n",
    "                batch = profile.get(\"batch_size\", batch)\n",
    "                client_lr = profile.get(\"lr\", client_lr)\n",
    "                dropout_prob = profile.get(\"dropout_prob\", dropout_prob)\n",
    "\n",
    "            if dropout_prob > 0 and random.random() < dropout_prob:\n",
    "                continue\n",
    "\n",
    "            client_dataset = Subset(train_dataset, shard)\n",
    "            local_state = local_train(\n",
    "                global_model,\n",
    "                client_dataset,\n",
    "                device,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch,\n",
    "                lr=client_lr,\n",
    "            )\n",
    "            client_states.append(local_state)\n",
    "            client_sizes.append(len(shard))\n",
    "\n",
    "        if not client_states:\n",
    "            print(f\"Round {r + 1}: no clients contributed updates; skipping aggregation.\")\n",
    "            continue\n",
    "\n",
    "        averaged_state = average_weights(client_states, client_sizes)\n",
    "        global_model.load_state_dict(averaged_state)\n",
    "        print(f\"Round {r + 1}/{rounds} finished. participants={len(client_states)}/{num_clients}\")\n",
    "\n",
    "    global_model.to(device)\n",
    "    global_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            preds = global_model(x).argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    accuracy = correct / total if total else 0.0\n",
    "    print(\"Global test acc:\", accuracy)\n",
    "    return global_model\n",
    "\n",
    "model_alpha_05 = federated_training(\n",
    "    train_dataset=train_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    partitions=dirichlet_partitions[0.5],\n",
    "    rounds=number_of_rounds,\n",
    "    device=\"mps\",\n",
    "    q=1.0,\n",
    "    num_classes=100,\n",
    ")\n",
    "\n",
    "hetero_profiles = {\n",
    "    0: {\"epochs\": 1, \"batch_size\": 32, \"lr\": 0.005, \"dropout_prob\": 0.10},\n",
    "    1: {\"epochs\": 1, \"batch_size\": 32, \"lr\": 0.005},\n",
    "    2: {\"epochs\": 1, \"batch_size\": 16, \"lr\": 0.005},\n",
    "    3: {\"dropout_prob\": 0.30},\n",
    "}\n",
    "\n",
    "global_model = federated_training(\n",
    "    train_dataset=train_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    partitions=dirichlet_partitions[0.1],\n",
    "    rounds=number_of_rounds,\n",
    "    device=\"mps\",\n",
    "    q=0.5,\n",
    "    num_classes=100,\n",
    "    hetero_profiles=hetero_profiles,\n",
    ")\n",
    "print(global_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl_privacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
