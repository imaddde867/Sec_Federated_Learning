{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "426f71b6-ed5c-47f9-9a35-f84920d83a5e",
   "metadata": {},
   "source": [
    "# Federated Averaging (FedAvg) baseline on CIFAR-100\n",
    "- Dirichlet non-IID partitioning\n",
    "- Partial client participation\n",
    "- Optional heterogeneity (per-client batch size / epochs / lr)\n",
    "## Imports and simulation defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb67608-4ff3-48de-a18c-c521e3ed9f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "# Reproducibility\n",
    "rng_seed = 42\n",
    "random.seed(rng_seed)\n",
    "np.random.seed(rng_seed)\n",
    "torch.manual_seed(rng_seed)\n",
    "\n",
    "# Seed CUDA/MPS for GPU determinism\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(rng_seed)\n",
    "if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    torch.mps.manual_seed(rng_seed)\n",
    "\n",
    "# Device helper (prefers CUDA, then MPS, else CPU)\n",
    "def get_device(prefer: Optional[str] = None) -> torch.device:\n",
    "    if prefer == \"cuda\" and torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if prefer == \"mps\" and getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640693cd-8558-4039-94a6-73e1c7310bb8",
   "metadata": {},
   "source": [
    "### Data: CIFAR-100 loaders (train/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b4d32ec-704c-495b-8df7-aae896d1af55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar100(data_root: str = \"./data\"):\n",
    "    # Mild augmentation on train; standard normalization\n",
    "    mean = (0.5071, 0.4867, 0.4408)\n",
    "    std = (0.2675, 0.2565, 0.2761)\n",
    "\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "    test_tf = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "\n",
    "    train = datasets.CIFAR100(root=data_root, train=True, download=True, transform=train_tf)\n",
    "    test = datasets.CIFAR100(root=data_root, train=False, download=True, transform=test_tf)\n",
    "    return train, test  # mirrors your original intent. \n",
    "\n",
    "\n",
    "# %%\n",
    "# Targets accessor (handles .targets/.labels)\n",
    "def _get_targets(dataset) -> np.ndarray:\n",
    "    targets = getattr(dataset, \"targets\", None)\n",
    "    if targets is None:\n",
    "        targets = getattr(dataset, \"labels\", None)\n",
    "    if targets is None:\n",
    "        raise AttributeError(\"Dataset has no 'targets' or 'labels'.\")\n",
    "    return np.array(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f347b8f-a81e-4fbb-a9a2-5618ed7d6fbd",
   "metadata": {},
   "source": [
    "## Dirichlet non-IID split (returns dict: client_id -> list of indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e02627af-60ba-44f7-bf5f-00cd33ebb247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dirichlet_noniid_indices(dataset, num_clients: int, alpha: float, min_per_client: int = 10) -> Dict[int, List[int]]:\n",
    "    y = _get_targets(dataset)\n",
    "    num_classes = int(y.max()) + 1\n",
    "    idx_by_class = {c: np.where(y == c)[0] for c in range(num_classes)}\n",
    "    for c in idx_by_class:\n",
    "        np.random.shuffle(idx_by_class[c])\n",
    "\n",
    "    # Each class's indices are split among clients with Dirichlet(alpha)\n",
    "    client_indices = [[] for _ in range(num_clients)]\n",
    "    for c in range(num_classes):\n",
    "        idx_c = idx_by_class[c]\n",
    "        if len(idx_c) == 0:\n",
    "            continue\n",
    "        # Dirichlet draw for this class across clients\n",
    "        p = np.random.dirichlet([alpha] * num_clients)\n",
    "        # Proportions -> integer split (rounding by cumulative sums)\n",
    "        cuts = (np.cumsum(p) * len(idx_c)).astype(int)[:-1]\n",
    "        split = np.split(idx_c, cuts)\n",
    "        for i, shard in enumerate(split):\n",
    "            client_indices[i].extend(shard.tolist())\n",
    "\n",
    "    # Ensure minimum per client (fallback to random fill if some are tiny)\n",
    "    # This usually isn't necessary for reasonable alpha, but keeps loaders happy\n",
    "    pool = list(range(len(dataset)))\n",
    "    for i in range(num_clients):\n",
    "        if len(client_indices[i]) < min_per_client:\n",
    "            need = min_per_client - len(client_indices[i])\n",
    "            extra = np.random.choice(pool, size=need, replace=False).tolist()\n",
    "            client_indices[i].extend(extra)\n",
    "\n",
    "    # Shuffle each client's order (nicer batching)\n",
    "    for i in range(num_clients):\n",
    "        random.shuffle(client_indices[i])\n",
    "    return {i: client_indices[i] for i in range(num_clients)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1ab77a-7051-4c8a-a378-17572b213790",
   "metadata": {},
   "source": [
    "## Model: ResNet18 head for CIFAR-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b48ab7-a3ea-4203-b42d-de354332bd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes: int = 100) -> nn.Module:\n",
    "    # Ensure reproducible model initialization\n",
    "    torch.manual_seed(rng_seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(rng_seed)\n",
    "    model = models.resnet18(weights=None)  # no pretrained to avoid download in restricted envs\n",
    "    # CIFAR images are 3x32x32; torchvision ResNet expects 224x224,\n",
    "    # but it's fineâ€”ResNet is fully conv except FC. It still works on 32x32.\n",
    "    # Replace final FC layer to match number of classes\n",
    "    in_feats = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_feats, num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "# %%\n",
    "def evaluate(model: nn.Module, loader: DataLoader, device: torch.device) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_sum = 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss_sum += loss.item() * x.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += x.size(0)\n",
    "    avg_loss = loss_sum / max(1, total)\n",
    "    acc = correct / max(1, total)\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "BN_TYPES = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)\n",
    "\n",
    "def ensure_dir(path):\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# convert the model's state_dict (dictionary format) into a long vector.\n",
    "def flatten_state_dict(state):\n",
    "    parts = []  # store all flattened tensors\n",
    "    keys = []   # storage parameter name\n",
    "    shapes = [] # store the original shape\n",
    "    for k, v in state.items():\n",
    "        t = v.detach().cpu().reshape(-1) # transform each parameter tensor into a one-dimensional vector\n",
    "        parts.append(t)\n",
    "        keys.append(k)\n",
    "        shapes.append(v.shape)\n",
    "    return torch.cat(parts, dim=0), (keys, shapes)\n",
    "\n",
    "def save_client_update_with_gradients(exp_dir, round_idx, client_id, global_state, gradients,  local_state, shard_size, lr, epochs):\n",
    "    # 1. Create a folder for this round\n",
    "    ensure_dir(Path(exp_dir) / f\"round_{round_idx}\")\n",
    "\n",
    "    # 2. Flattening the local and global models\n",
    "    local_flat, template = flatten_state_dict(local_state)\n",
    "    global_flat, _ = flatten_state_dict(global_state)\n",
    "\n",
    "    # 3. Calculate parameter update (delta)\n",
    "    delta = (local_flat - global_flat).cpu()\n",
    "    \n",
    "    # 4. Package all information\n",
    "    payload = {\n",
    "        \"delta\": delta,\n",
    "        \"template\": template,\n",
    "        \"meta\": {\n",
    "            \"round\": round_idx,\n",
    "            \"client_id\": client_id,\n",
    "            \"shard_size\": shard_size,\n",
    "            \"lr\": lr,\n",
    "            \"epochs\": epochs,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    if gradients is not None:\n",
    "        grad_flat, grad_template = flatten_state_dict(gradients)\n",
    "        payload[\"gradients\"] = grad_flat\n",
    "        payload[\"grad_template\"] = grad_template\n",
    "    \n",
    "    # 5. Save as a .pt file\n",
    "    fname = Path(exp_dir) / f\"round_{round_idx}\" / f\"client_{client_id}.pt\"\n",
    "    torch.save(payload, fname)\n",
    "\n",
    "# %%\n",
    "def local_train(\n",
    "    global_model: nn.Module,\n",
    "    subset: Subset,\n",
    "    device: torch.device,\n",
    "    epochs: int = 1,\n",
    "    batch_size: int = 64,\n",
    "    lr: float = 0.01,\n",
    "):\n",
    "    # 1. local copy\n",
    "    model = copy.deepcopy(global_model).to(device)\n",
    "\n",
    "    # 2. dataloader (same as before)\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = rng_seed + worker_id\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    g = torch.Generator(); g.manual_seed(rng_seed)\n",
    "    loader = DataLoader(\n",
    "        subset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=2, pin_memory=False,\n",
    "        worker_init_fn=seed_worker, generator=g\n",
    "    )\n",
    "\n",
    "    # 3. setup\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "    bn_layers = [m for m in model.modules() if isinstance(m, BN_TYPES)]\n",
    "\n",
    "    # New: gradient collectors (init ONCE, before loop)\n",
    "    grad_sums = {n: torch.zeros_like(p, device=device) for n, p in model.named_parameters()}\n",
    "    grad_steps = 0\n",
    "\n",
    "    # 4. train and accumulate grads INSIDE the existing loop\n",
    "    model.train()\n",
    "    for _ in range(max(1, epochs)):\n",
    "        for x, y in loader:\n",
    "            x = x.to(device); y = y.to(device)\n",
    "\n",
    "            # (your BN small-batch toggle)\n",
    "            toggled = []\n",
    "            if x.size(0) < 2:\n",
    "                for layer in bn_layers:\n",
    "                    if layer.training:\n",
    "                        toggled.append(layer); layer.eval()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "\n",
    "            # New: accumulate per-batch gradients\n",
    "            for n, p in model.named_parameters():\n",
    "                if p.grad is not None:\n",
    "                    grad_sums[n] += p.grad.detach()\n",
    "            grad_steps += 1\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            for layer in toggled:\n",
    "                layer.train()\n",
    "\n",
    "    # 5. collect weights after training\n",
    "    local_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "    # 6. align gradients to state_dict keys (for flatten_state_dict)\n",
    "    gradients = {}\n",
    "    sd = model.state_dict()\n",
    "    for k, v in sd.items():\n",
    "        if k in grad_sums:  # trainable param\n",
    "            gradients[k] = (grad_sums[k] / max(1, grad_steps)).detach().cpu()\n",
    "        else:               # buffers like BN running stats\n",
    "            gradients[k] = torch.zeros_like(v).detach().cpu()\n",
    "\n",
    "    return local_state, gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b2afc1-e8b4-4d9f-ba4f-0ade607afdfb",
   "metadata": {},
   "source": [
    "## Weighted model averaging (FedAvg) with checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2643eeb-173f-4794-a397-040687ca1ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_weights(weight_list, sizes):\n",
    "    if not weight_list:\n",
    "        raise ValueError(\"No client weights provided.\")\n",
    "    if len(weight_list) != len(sizes):\n",
    "        raise ValueError(\"weights and sizes mismatch\")\n",
    "    total = float(sum(sizes))\n",
    "    avg = {k: torch.zeros_like(v) for k, v in weight_list[0].items()}\n",
    "\n",
    "    for wi, si in zip(weight_list, sizes):\n",
    "        w = si / total\n",
    "        for k in avg.keys():\n",
    "            if avg[k].dtype.is_floating_point:      \n",
    "                avg[k] += wi[k].float() * w\n",
    "            else:\n",
    "                avg[k] = wi[k].clone()              \n",
    "\n",
    "    return avg # mirrors safety we intend. :contentReference[oaicite:2]{index=2}\n",
    "\n",
    "# %%\n",
    "def federated_training(\n",
    "    train_dataset,\n",
    "    test_dataset,\n",
    "    partitions: Dict[int, List[int]],\n",
    "    rounds: int = 10,\n",
    "    local_epochs: int = 1,\n",
    "    device: str = \"cpu\",\n",
    "    q: float = 1.0,  # participation rate per round\n",
    "    num_classes: int = 100,\n",
    "    batch_size: int = 64,\n",
    "    lr: float = 0.01,\n",
    "    hetero_profiles: Optional[Dict[int, Dict[str, float]]] = None,  # per-client overrides\n",
    "    exp_id: str = \"exp_test\",        # experiment name (used as folder)\n",
    "    reports_dir: str = \"./reports\",  # base output folder for all experiments\n",
    "    rng_seed: Optional[int] = None,  # optional: record seed in config\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the global model using FedAvg over 'rounds' communication rounds.\n",
    "    \"\"\"\n",
    "# Validate partitions\n",
    "    num_clients = len(partitions)\n",
    "    if num_clients == 0:\n",
    "        raise ValueError(\"No clients available for federated training.\")\n",
    "    device = get_device(device)\n",
    "\n",
    "    # Build global model\n",
    "    global_model = build_model(num_classes=num_classes).to(device)\n",
    "\n",
    "    # Resolve and create experiment output folder\n",
    "    exp_dir = Path(reports_dir) / exp_id\n",
    "    ensure_dir(exp_dir)\n",
    "\n",
    "    # Save experiment metadata once at the start\n",
    "    meta_info = {\n",
    "        \"num_clients\": num_clients,\n",
    "        \"rounds\": rounds,\n",
    "        \"local_epochs\": local_epochs,\n",
    "        \"q\": q,\n",
    "        \"num_classes\": num_classes,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"lr\": lr,\n",
    "        \"hetero_profiles\": hetero_profiles,\n",
    "        \"seed\": rng_seed,\n",
    "    }\n",
    "    torch.save(meta_info, exp_dir / \"experiment_config.pt\")\n",
    "\n",
    "    # Test loader (global test set)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=2)\n",
    "\n",
    "    history = {\"round\": [], \"test_loss\": [], \"test_acc\": [], \"selected\": []}\n",
    "\n",
    "    for r in range(1, rounds + 1):\n",
    "        # Client sampling\n",
    "        m = max(1, int(math.ceil(q * num_clients)))\n",
    "        selected = sorted(random.sample(range(num_clients), m))\n",
    "\n",
    "        global_state_for_round = {k: v.detach().cpu() for k, v in global_model.state_dict().items()}\n",
    "\n",
    "        # Local updates\n",
    "        local_weights = []\n",
    "        local_sizes = []\n",
    "        for cid in selected:\n",
    "            idxs = partitions[cid]\n",
    "            subset = Subset(train_dataset, idxs)\n",
    "\n",
    "            # Per-client heterogeneity overrides if provided\n",
    "            ep = local_epochs\n",
    "            bs = batch_size\n",
    "            lr_i = lr\n",
    "            dropout_prob = 0.0\n",
    "            if hetero_profiles and cid in hetero_profiles:\n",
    "                ep = int(hetero_profiles[cid].get(\"epochs\", ep))\n",
    "                bs = int(hetero_profiles[cid].get(\"batch_size\", bs))\n",
    "                lr_i = float(hetero_profiles[cid].get(\"lr\", lr_i))\n",
    "                dropout_prob = float(hetero_profiles[cid].get(\"dropout_prob\", 0.0))\n",
    "\n",
    "            if dropout_prob > 0 and random.random() < dropout_prob:\n",
    "                continue\n",
    "\n",
    "            wi, grads = local_train(global_model, subset, device, epochs=ep, batch_size=bs, lr=lr_i)\n",
    "            local_weights.append(wi)\n",
    "            local_sizes.append(len(subset))\n",
    "\n",
    "            # Persist this client's update (flat delta + mapping template + meta)\n",
    "            save_client_update_with_gradients(\n",
    "                exp_dir=str(exp_dir),\n",
    "                round_idx=r - 1,\n",
    "                client_id=cid,\n",
    "                global_state=global_state_for_round,\n",
    "                local_state=wi,\n",
    "                gradients=grads,\n",
    "                shard_size=len(subset),\n",
    "                lr=lr_i,\n",
    "                epochs=ep,\n",
    "            )\n",
    "\n",
    "        if not local_weights:\n",
    "            print(f\"[Round {r:03d}] No clients contributed; skipping aggregation.\")\n",
    "            continue\n",
    "\n",
    "        # FedAvg\n",
    "        new_state = average_weights(local_weights, local_sizes)\n",
    "        global_model.load_state_dict(new_state)\n",
    "\n",
    "        # Evaluate global model\n",
    "        test_loss, test_acc = evaluate(global_model, test_loader, device)\n",
    "        history[\"round\"].append(r)\n",
    "        history[\"test_loss\"].append(test_loss)\n",
    "        history[\"test_acc\"].append(test_acc)\n",
    "        history[\"selected\"].append(selected)\n",
    "\n",
    "        print(f\"[Round {r:03d}] Test loss: {test_loss:.4f} | Test acc: {test_acc*100:.2f}% | Clients: {selected}\")\n",
    "\n",
    "    # Save final global model inside the experiment folder\n",
    "    final_model_path = exp_dir / \"final_global_model.pt\"\n",
    "    torch.save(global_model.state_dict(), final_model_path)\n",
    "    print(f\"Final global model saved to {final_model_path}\")\n",
    "\n",
    "    return global_model, history  # same signature goal you had. :contentReference[oaicite:3]{index=3}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb72276a-9d1b-47fe-8d8d-014832f70ea4",
   "metadata": {},
   "source": [
    "## Final "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e155dac-a8ee-45c5-bd73-f1a63236d331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169M/169M [00:24<00:00, 6.88MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round 001] Test loss: 4.7216 | Test acc: 1.00% | Clients: [0, 2, 3, 4, 5, 6, 7, 9]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 31\u001b[0m\n\u001b[1;32m     27\u001b[0m hetero_profiles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# --- Federated training ---\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# First training iteration (alpha=0.5, baseline)\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m model, hist \u001b[38;5;241m=\u001b[39m \u001b[43mfederated_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartitions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumber_of_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_pref\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparticipation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhetero_profiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhetero_profiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43malpha_0.5_baseline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreports_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./reports\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrng_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# --- Second experiment: Heterogeneous (alpha=0.1) ---\u001b[39;00m\n\u001b[1;32m     49\u001b[0m partitions_hetero \u001b[38;5;241m=\u001b[39m dirichlet_noniid_indices(\n\u001b[1;32m     50\u001b[0m     train_dataset, num_clients\u001b[38;5;241m=\u001b[39mnumber_of_clients, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m     51\u001b[0m )\n",
      "Cell \u001b[0;32mIn[5], line 99\u001b[0m, in \u001b[0;36mfederated_training\u001b[0;34m(train_dataset, test_dataset, partitions, rounds, local_epochs, device, q, num_classes, batch_size, lr, hetero_profiles, exp_id, reports_dir, rng_seed)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dropout_prob \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m dropout_prob:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m wi \u001b[38;5;241m=\u001b[39m \u001b[43mlocal_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_i\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m local_weights\u001b[38;5;241m.\u001b[39mappend(wi)\n\u001b[1;32m    101\u001b[0m local_sizes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(subset))\n",
      "Cell \u001b[0;32mIn[4], line 110\u001b[0m, in \u001b[0;36mlocal_train\u001b[0;34m(global_model, subset, device, epochs, batch_size, lr)\u001b[0m\n\u001b[1;32m    108\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m    109\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, y)\n\u001b[0;32m--> 110\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m toggled:\n",
      "File \u001b[0;32m~/miniconda3/envs/fl_privacy/lib/python3.9/site-packages/torch/_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[0;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/fl_privacy/lib/python3.9/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/fl_privacy/lib/python3.9/site-packages/torch/autograd/graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # --- Config (kept close to your defaults) ---\n",
    "    number_of_clients = 10\n",
    "    number_of_rounds = 20\n",
    "    local_epochs = 1            # start small for smoke tests; raise later\n",
    "    participation = 0.75        # fraction of clients per round\n",
    "    dirichlet_alpha = 0.5       # non-IID strength (lower => more skew)\n",
    "    batch_size = 64\n",
    "    lr = 0.01\n",
    "    device_pref = None          # \"cuda\" | \"mps\" | \"cpu\" | None (auto)\n",
    "\n",
    "    # (These reflect the same values/types you were using.)\n",
    "\n",
    "    # --- Data ---\n",
    "    train_dataset, test_dataset = load_cifar100()\n",
    "\n",
    "    # --- Partitioning ---\n",
    "    partitions = dirichlet_noniid_indices(\n",
    "        train_dataset, num_clients=number_of_clients, alpha=dirichlet_alpha\n",
    "    )\n",
    "\n",
    "    # Optional: example heterogeneity profile\n",
    "    # hetero_profiles = {\n",
    "    #     0: {\"epochs\": 2, \"batch_size\": 32, \"lr\": 0.02},\n",
    "    #     3: {\"epochs\": 1, \"batch_size\": 128, \"lr\": 0.005},\n",
    "    # }\n",
    "    hetero_profiles = None\n",
    "\n",
    "    # --- Federated training ---\n",
    "    # First training iteration (alpha=0.5, baseline)\n",
    "    model, hist = federated_training(\n",
    "        train_dataset=train_dataset,\n",
    "        test_dataset=test_dataset,\n",
    "        partitions=partitions,\n",
    "        rounds=number_of_rounds,\n",
    "        local_epochs=local_epochs,\n",
    "        device=device_pref or \"auto\",\n",
    "        q=participation,\n",
    "        num_classes=100,\n",
    "        batch_size=batch_size,\n",
    "        lr=lr,\n",
    "        hetero_profiles=hetero_profiles,\n",
    "        exp_id=\"alpha_0.5_baseline\",\n",
    "        reports_dir=\"./reports\",\n",
    "        rng_seed=rng_seed,\n",
    "    )\n",
    "\n",
    "    # --- Second experiment: Heterogeneous (alpha=0.1) ---\n",
    "    partitions_hetero = dirichlet_noniid_indices(\n",
    "        train_dataset, num_clients=number_of_clients, alpha=0.1\n",
    "    )\n",
    "\n",
    "    hetero_profiles = {\n",
    "        0: {\"epochs\": 2, \"batch_size\": 48, \"lr\": 0.0075, \"dropout_prob\": 0.10},\n",
    "        1: {\"epochs\": 1, \"batch_size\": 32, \"lr\": 0.005},\n",
    "        2: {\"epochs\": 1, \"batch_size\": 24, \"lr\": 0.006},\n",
    "        3: {\"dropout_prob\": 0.25},\n",
    "        4: {\"epochs\": 2, \"batch_size\": 64, \"lr\": 0.012},\n",
    "        5: {\"dropout_prob\": 0.15, \"lr\": 0.008},\n",
    "    }\n",
    "\n",
    "    # Second training session (alpha=0.1, hetero)\n",
    "    hetero_model, hist_hetero = federated_training(\n",
    "        train_dataset=train_dataset,\n",
    "        test_dataset=test_dataset,\n",
    "        partitions=partitions_hetero,\n",
    "        rounds=number_of_rounds,\n",
    "        local_epochs=local_epochs + 1,\n",
    "        device=device_pref or \"auto\",\n",
    "        q=0.6,\n",
    "        num_classes=100,\n",
    "        batch_size=batch_size,\n",
    "        lr=lr,\n",
    "        hetero_profiles=hetero_profiles,\n",
    "        exp_id=\"alpha_0.1_hetero\",\n",
    "        reports_dir=\"./reports\",\n",
    "        rng_seed=rng_seed,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5546f63b",
   "metadata": {},
   "source": [
    "## Metrics to export \n",
    "\n",
    "metrics_to_export = {\n",
    "    'round': round_num,\n",
    "    'participating_clients': client_ids,\n",
    "    \n",
    "    # Per-client exports\n",
    "    'client_metrics': {\n",
    "        client_id: {\n",
    "            'gradient_norm': float,\n",
    "            'per_layer_norms': dict,\n",
    "            'batch_size': int,\n",
    "            'local_epochs': int,\n",
    "            'learning_rate': float,\n",
    "            'num_samples': int,\n",
    "            'class_distribution': dict,\n",
    "            'local_loss': float,\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Global state\n",
    "    'global_model_state': state_dict,  # or checkpoint path\n",
    "    'global_accuracy': float,\n",
    "    'global_loss': float,\n",
    "    \n",
    "    # For reconstruction\n",
    "    'raw_gradients': {client_id: gradient_dict},  # KEY for breaching\n",
    "    'model_updates': {client_id: update_dict},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ae2263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics extracted for 8 clients.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import copy\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# reconstruct final state and global metrics\n",
    "global_loss, global_acc = evaluate(model, DataLoader(test_dataset, batch_size=256, shuffle=False), get_device())\n",
    "\n",
    "# container for metrics\n",
    "metrics_to_export = {\n",
    "    \"round\": number_of_rounds,\n",
    "    \"participating_clients\": hist[\"selected\"][-1],\n",
    "    \"client_metrics\": {},\n",
    "    \"global_model_state\": copy.deepcopy(model.state_dict()),  # could also save to disk\n",
    "    \"global_accuracy\": float(global_acc),\n",
    "    \"global_loss\": float(global_loss),\n",
    "    \"raw_gradients\": {},\n",
    "    \"model_updates\": {},\n",
    "}\n",
    "\n",
    "# --- compute per-client metrics on the final round participants ---\n",
    "device = get_device()\n",
    "global_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for cid in hist[\"selected\"][-1]:\n",
    "    idxs = partitions[cid]\n",
    "    subset = Subset(train_dataset, idxs)\n",
    "    loader = DataLoader(subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    # clone model to compute client gradients from final global weights\n",
    "    client_model = build_model(num_classes=100).to(device)\n",
    "    client_model.load_state_dict(global_state)\n",
    "    client_model.train()\n",
    "\n",
    "    optimizer = torch.optim.SGD(client_model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "    # one batch to approximate gradient norm\n",
    "    x, y = next(iter(loader))\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    logits = client_model(x)\n",
    "    loss = criterion(logits, y)\n",
    "    loss.backward()\n",
    "\n",
    "    grad_dict = {n: p.grad.detach().cpu().clone() for n, p in client_model.named_parameters() if p.grad is not None}\n",
    "    grad_norm = torch.sqrt(sum(torch.norm(g)**2 for g in grad_dict.values())).item()\n",
    "    per_layer_norms = {n: torch.norm(g).item() for n, g in grad_dict.items()}\n",
    "\n",
    "    # simulate a local update (1 epoch) to get model delta\n",
    "    local_weights = local_train(model, subset, device, epochs=1, batch_size=batch_size, lr=lr)\n",
    "    update_dict = {\n",
    "    k: local_weights[k].cpu() - global_state[k].detach().cpu()\n",
    "    for k in global_state.keys()\n",
    "    }\n",
    "    # class distribution for this client\n",
    "    y_subset = [train_dataset.targets[i] for i in idxs]\n",
    "    class_counts = dict(zip(*np.unique(y_subset, return_counts=True)))\n",
    "\n",
    "    metrics_to_export[\"client_metrics\"][cid] = {\n",
    "        \"gradient_norm\": float(grad_norm),\n",
    "        \"per_layer_norms\": per_layer_norms,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"local_epochs\": local_epochs,\n",
    "        \"learning_rate\": lr,\n",
    "        \"num_samples\": len(subset),\n",
    "        \"class_distribution\": class_counts,\n",
    "        \"local_loss\": float(loss.item()),\n",
    "    }\n",
    "    metrics_to_export[\"raw_gradients\"][cid] = grad_dict\n",
    "    metrics_to_export[\"model_updates\"][cid] = update_dict\n",
    "\n",
    "print(\"Metrics extracted for\", len(metrics_to_export[\"client_metrics\"]), \"clients.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bdaba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global accuracy: 0.3223\n",
      "Global loss: 2.683524996185303\n",
      "Round: 20\n",
      "Clients: [0, 1, 2, 3, 4, 5, 7, 9]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"Global accuracy:\", metrics_to_export[\"global_accuracy\"])\n",
    "print(\"Global loss:\", metrics_to_export[\"global_loss\"])\n",
    "print(\"Round:\", metrics_to_export[\"round\"])\n",
    "print(\"Clients:\", metrics_to_export[\"participating_clients\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1212fd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Client 0:\n",
      "  samples = 4860\n",
      "  local_loss = 3.0172\n",
      "  grad_norm = 5.83\n",
      "  top-3 per-layer norms = [('conv1.weight', 1.475261926651001), ('bn1.weight', 0.1264485865831375), ('bn1.bias', 0.07532992959022522)]\n",
      "\n",
      "Client 1:\n",
      "  samples = 5014\n",
      "  local_loss = 2.6070\n",
      "  grad_norm = 4.69\n",
      "  top-3 per-layer norms = [('conv1.weight', 1.169519066810608), ('bn1.weight', 0.12056384980678558), ('bn1.bias', 0.06122458353638649)]\n",
      "\n",
      "Client 2:\n",
      "  samples = 5582\n",
      "  local_loss = 2.4577\n",
      "  grad_norm = 4.97\n",
      "  top-3 per-layer norms = [('conv1.weight', 1.2443177700042725), ('bn1.weight', 0.09924472868442535), ('bn1.bias', 0.0552186593413353)]\n",
      "\n",
      "Client 3:\n",
      "  samples = 4299\n",
      "  local_loss = 2.8635\n",
      "  grad_norm = 5.41\n",
      "  top-3 per-layer norms = [('conv1.weight', 1.4312726259231567), ('bn1.weight', 0.12580248713493347), ('bn1.bias', 0.053155455738306046)]\n",
      "\n",
      "Client 4:\n",
      "  samples = 4640\n",
      "  local_loss = 3.1361\n",
      "  grad_norm = 4.92\n",
      "  top-3 per-layer norms = [('conv1.weight', 1.0570460557937622), ('bn1.weight', 0.10088635236024857), ('bn1.bias', 0.06778769195079803)]\n",
      "\n",
      "Client 5:\n",
      "  samples = 5622\n",
      "  local_loss = 3.0579\n",
      "  grad_norm = 5.73\n",
      "  top-3 per-layer norms = [('conv1.weight', 1.2626618146896362), ('bn1.weight', 0.11588869243860245), ('bn1.bias', 0.07169351726770401)]\n",
      "\n",
      "Client 7:\n",
      "  samples = 4753\n",
      "  local_loss = 2.4740\n",
      "  grad_norm = 5.60\n",
      "  top-3 per-layer norms = [('conv1.weight', 1.2734787464141846), ('bn1.weight', 0.15708310902118683), ('bn1.bias', 0.06478703022003174)]\n",
      "\n",
      "Client 9:\n",
      "  samples = 4966\n",
      "  local_loss = 2.8763\n",
      "  grad_norm = 5.95\n",
      "  top-3 per-layer norms = [('conv1.weight', 1.7246252298355103), ('bn1.weight', 0.14023591578006744), ('bn1.bias', 0.07096107304096222)]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for cid, info in metrics_to_export[\"client_metrics\"].items():\n",
    "    print(f\"\\nClient {cid}:\")\n",
    "    print(f\"  samples = {info['num_samples']}\")\n",
    "    print(f\"  local_loss = {info['local_loss']:.4f}\")\n",
    "    print(f\"  grad_norm = {info['gradient_norm']:.2f}\")\n",
    "    print(f\"  top-3 per-layer norms = {list(info['per_layer_norms'].items())[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53771e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'batch_size': 64,\n",
      "     'class_distribution': {np.int64(0): np.int64(5),\n",
      "                            np.int64(1): np.int64(160),\n",
      "                            np.int64(3): np.int64(50),\n",
      "                            np.int64(4): np.int64(21),\n",
      "                            np.int64(5): np.int64(78),\n",
      "                            np.int64(6): np.int64(90),\n",
      "                            np.int64(7): np.int64(267),\n",
      "                            np.int64(8): np.int64(29),\n",
      "                            np.int64(9): np.int64(26),\n",
      "                            np.int64(10): np.int64(96),\n",
      "                            np.int64(11): np.int64(74),\n",
      "                            np.int64(12): np.int64(24),\n",
      "                            np.int64(13): np.int64(56),\n",
      "                            np.int64(14): np.int64(50),\n",
      "                            np.int64(15): np.int64(3),\n",
      "                            np.int64(16): np.int64(27),\n",
      "                            np.int64(17): np.int64(57),\n",
      "                            np.int64(18): np.int64(11),\n",
      "                            np.int64(19): np.int64(163),\n",
      "                            np.int64(20): np.int64(31),\n",
      "                            np.int64(21): np.int64(7),\n",
      "                            np.int64(22): np.int64(15),\n",
      "                            np.int64(23): np.int64(16),\n",
      "                            np.int64(24): np.int64(73),\n",
      "                            np.int64(25): np.int64(60),\n",
      "                            np.int64(27): np.int64(63),\n",
      "                            np.int64(29): np.int64(19),\n",
      "                            np.int64(32): np.int64(5),\n",
      "                            np.int64(33): np.int64(254),\n",
      "                            np.int64(35): np.int64(38),\n",
      "                            np.int64(36): np.int64(11),\n",
      "                            np.int64(38): np.int64(171),\n",
      "                            np.int64(39): np.int64(7),\n",
      "                            np.int64(40): np.int64(79),\n",
      "                            np.int64(41): np.int64(9),\n",
      "                            np.int64(42): np.int64(103),\n",
      "                            np.int64(43): np.int64(4),\n",
      "                            np.int64(44): np.int64(51),\n",
      "                            np.int64(45): np.int64(3),\n",
      "                            np.int64(46): np.int64(9),\n",
      "                            np.int64(47): np.int64(12),\n",
      "                            np.int64(48): np.int64(20),\n",
      "                            np.int64(49): np.int64(14),\n",
      "                            np.int64(50): np.int64(4),\n",
      "                            np.int64(51): np.int64(10),\n",
      "                            np.int64(53): np.int64(1),\n",
      "                            np.int64(54): np.int64(31),\n",
      "                            np.int64(55): np.int64(11),\n",
      "                            np.int64(56): np.int64(38),\n",
      "                            np.int64(57): np.int64(2),\n",
      "                            np.int64(58): np.int64(13),\n",
      "                            np.int64(59): np.int64(83),\n",
      "                            np.int64(60): np.int64(41),\n",
      "                            np.int64(61): np.int64(11),\n",
      "                            np.int64(62): np.int64(59),\n",
      "                            np.int64(63): np.int64(60),\n",
      "                            np.int64(64): np.int64(97),\n",
      "                            np.int64(65): np.int64(19),\n",
      "                            np.int64(66): np.int64(67),\n",
      "                            np.int64(67): np.int64(4),\n",
      "                            np.int64(69): np.int64(117),\n",
      "                            np.int64(70): np.int64(137),\n",
      "                            np.int64(71): np.int64(60),\n",
      "                            np.int64(72): np.int64(6),\n",
      "                            np.int64(73): np.int64(255),\n",
      "                            np.int64(74): np.int64(170),\n",
      "                            np.int64(75): np.int64(126),\n",
      "                            np.int64(76): np.int64(3),\n",
      "                            np.int64(77): np.int64(3),\n",
      "                            np.int64(78): np.int64(58),\n",
      "                            np.int64(79): np.int64(13),\n",
      "                            np.int64(80): np.int64(56),\n",
      "                            np.int64(81): np.int64(102),\n",
      "                            np.int64(82): np.int64(90),\n",
      "                            np.int64(83): np.int64(93),\n",
      "                            np.int64(84): np.int64(81),\n",
      "                            np.int64(85): np.int64(18),\n",
      "                            np.int64(86): np.int64(169),\n",
      "                            np.int64(89): np.int64(129),\n",
      "                            np.int64(90): np.int64(31),\n",
      "                            np.int64(91): np.int64(22),\n",
      "                            np.int64(92): np.int64(8),\n",
      "                            np.int64(93): np.int64(21),\n",
      "                            np.int64(94): np.int64(6),\n",
      "                            np.int64(95): np.int64(11),\n",
      "                            np.int64(96): np.int64(105),\n",
      "                            np.int64(97): np.int64(28),\n",
      "                            np.int64(98): np.int64(9),\n",
      "                            np.int64(99): np.int64(21)},\n",
      "     'gradient_norm': 5.834565162658691,\n",
      "     'learning_rate': 0.01,\n",
      "     'local_epochs': 1,\n",
      "     'local_loss': 3.01720929145813,\n",
      "     'num_samples': 4860,\n",
      "     'per_layer_norms': {'bn1.bias': 0.07532992959022522,\n",
      "                         'bn1.weight': 0.1264485865831375,\n",
      "                         'conv1.weight': 1.475261926651001,\n",
      "                         'fc.bias': 0.15097637474536896,\n",
      "                         'fc.weight': 2.721064567565918,\n",
      "                         'layer1.0.bn1.bias': 0.06970631331205368,\n",
      "                         'layer1.0.bn1.weight': 0.0753355547785759,\n",
      "                         'layer1.0.bn2.bias': 0.04562889784574509,\n",
      "                         'layer1.0.bn2.weight': 0.08057960122823715,\n",
      "                         'layer1.0.conv1.weight': 1.129195213317871,\n",
      "                         'layer1.0.conv2.weight': 1.0432254076004028,\n",
      "                         'layer1.1.bn1.bias': 0.04874301329255104,\n",
      "                         'layer1.1.bn1.weight': 0.05861763283610344,\n",
      "                         'layer1.1.bn2.bias': 0.03015117160975933,\n",
      "                         'layer1.1.bn2.weight': 0.0566389262676239,\n",
      "                         'layer1.1.conv1.weight': 0.9460119009017944,\n",
      "                         'layer1.1.conv2.weight': 0.8806372880935669,\n",
      "                         'layer2.0.bn1.bias': 0.05180972069501877,\n",
      "                         'layer2.0.bn1.weight': 0.05651677027344704,\n",
      "                         'layer2.0.bn2.bias': 0.04631428420543671,\n",
      "                         'layer2.0.bn2.weight': 0.06717883795499802,\n",
      "                         'layer2.0.conv1.weight': 1.1972055435180664,\n",
      "                         'layer2.0.conv2.weight': 1.3109309673309326,\n",
      "                         'layer2.0.downsample.0.weight': 0.5149397253990173,\n",
      "                         'layer2.0.downsample.1.bias': 0.04631428420543671,\n",
      "                         'layer2.0.downsample.1.weight': 0.06052160635590553,\n",
      "                         'layer2.1.bn1.bias': 0.04679363965988159,\n",
      "                         'layer2.1.bn1.weight': 0.05826737359166145,\n",
      "                         'layer2.1.bn2.bias': 0.03408444672822952,\n",
      "                         'layer2.1.bn2.weight': 0.05661188066005707,\n",
      "                         'layer2.1.conv1.weight': 1.2522435188293457,\n",
      "                         'layer2.1.conv2.weight': 1.1408905982971191,\n",
      "                         'layer3.0.bn1.bias': 0.043737445026636124,\n",
      "                         'layer3.0.bn1.weight': 0.05395858734846115,\n",
      "                         'layer3.0.bn2.bias': 0.03646434471011162,\n",
      "                         'layer3.0.bn2.weight': 0.057434435933828354,\n",
      "                         'layer3.0.conv1.weight': 1.5867993831634521,\n",
      "                         'layer3.0.conv2.weight': 1.4280022382736206,\n",
      "                         'layer3.0.downsample.0.weight': 0.5388096570968628,\n",
      "                         'layer3.0.downsample.1.bias': 0.03646434471011162,\n",
      "                         'layer3.0.downsample.1.weight': 0.04767106473445892,\n",
      "                         'layer3.1.bn1.bias': 0.028676893562078476,\n",
      "                         'layer3.1.bn1.weight': 0.028136570006608963,\n",
      "                         'layer3.1.bn2.bias': 0.022345537319779396,\n",
      "                         'layer3.1.bn2.weight': 0.036439817398786545,\n",
      "                         'layer3.1.conv1.weight': 1.1551029682159424,\n",
      "                         'layer3.1.conv2.weight': 1.0108799934387207,\n",
      "                         'layer4.0.bn1.bias': 0.031086299568414688,\n",
      "                         'layer4.0.bn1.weight': 0.03582274913787842,\n",
      "                         'layer4.0.bn2.bias': 0.06717338413000107,\n",
      "                         'layer4.0.bn2.weight': 0.06562190502882004,\n",
      "                         'layer4.0.conv1.weight': 1.3423603773117065,\n",
      "                         'layer4.0.conv2.weight': 1.3504948616027832,\n",
      "                         'layer4.0.downsample.0.weight': 0.814007580280304,\n",
      "                         'layer4.0.downsample.1.bias': 0.06717338413000107,\n",
      "                         'layer4.0.downsample.1.weight': 0.07109031081199646,\n",
      "                         'layer4.1.bn1.bias': 0.028254060074687004,\n",
      "                         'layer4.1.bn1.weight': 0.03144112974405289,\n",
      "                         'layer4.1.bn2.bias': 0.0772344097495079,\n",
      "                         'layer4.1.bn2.weight': 0.0485898032784462,\n",
      "                         'layer4.1.conv1.weight': 1.1306838989257812,\n",
      "                         'layer4.1.conv2.weight': 1.1052205562591553}},\n",
      " 1: {'batch_size': 64,\n",
      "     'class_distribution': {np.int64(0): np.int64(2),\n",
      "                            np.int64(1): np.int64(38),\n",
      "                            np.int64(2): np.int64(38),\n",
      "                            np.int64(3): np.int64(11),\n",
      "                            np.int64(6): np.int64(138),\n",
      "                            np.int64(7): np.int64(41),\n",
      "                            np.int64(8): np.int64(1),\n",
      "                            np.int64(9): np.int64(8),\n",
      "                            np.int64(10): np.int64(3),\n",
      "                            np.int64(11): np.int64(116),\n",
      "                            np.int64(12): np.int64(205),\n",
      "                            np.int64(13): np.int64(40),\n",
      "                            np.int64(14): np.int64(101),\n",
      "                            np.int64(15): np.int64(27),\n",
      "                            np.int64(16): np.int64(111),\n",
      "                            np.int64(17): np.int64(212),\n",
      "                            np.int64(18): np.int64(3),\n",
      "                            np.int64(19): np.int64(2),\n",
      "                            np.int64(20): np.int64(92),\n",
      "                            np.int64(21): np.int64(84),\n",
      "                            np.int64(22): np.int64(38),\n",
      "                            np.int64(23): np.int64(150),\n",
      "                            np.int64(24): np.int64(150),\n",
      "                            np.int64(25): np.int64(56),\n",
      "                            np.int64(26): np.int64(8),\n",
      "                            np.int64(27): np.int64(6),\n",
      "                            np.int64(28): np.int64(1),\n",
      "                            np.int64(30): np.int64(46),\n",
      "                            np.int64(31): np.int64(89),\n",
      "                            np.int64(32): np.int64(71),\n",
      "                            np.int64(33): np.int64(6),\n",
      "                            np.int64(34): np.int64(52),\n",
      "                            np.int64(35): np.int64(34),\n",
      "                            np.int64(36): np.int64(59),\n",
      "                            np.int64(37): np.int64(36),\n",
      "                            np.int64(38): np.int64(102),\n",
      "                            np.int64(39): np.int64(7),\n",
      "                            np.int64(40): np.int64(4),\n",
      "                            np.int64(41): np.int64(4),\n",
      "                            np.int64(42): np.int64(28),\n",
      "                            np.int64(43): np.int64(20),\n",
      "                            np.int64(44): np.int64(95),\n",
      "                            np.int64(45): np.int64(9),\n",
      "                            np.int64(46): np.int64(9),\n",
      "                            np.int64(47): np.int64(23),\n",
      "                            np.int64(50): np.int64(66),\n",
      "                            np.int64(51): np.int64(3),\n",
      "                            np.int64(52): np.int64(15),\n",
      "                            np.int64(53): np.int64(59),\n",
      "                            np.int64(54): np.int64(14),\n",
      "                            np.int64(55): np.int64(74),\n",
      "                            np.int64(56): np.int64(88),\n",
      "                            np.int64(57): np.int64(5),\n",
      "                            np.int64(58): np.int64(225),\n",
      "                            np.int64(59): np.int64(27),\n",
      "                            np.int64(60): np.int64(46),\n",
      "                            np.int64(61): np.int64(2),\n",
      "                            np.int64(62): np.int64(62),\n",
      "                            np.int64(63): np.int64(51),\n",
      "                            np.int64(64): np.int64(232),\n",
      "                            np.int64(65): np.int64(151),\n",
      "                            np.int64(66): np.int64(61),\n",
      "                            np.int64(68): np.int64(77),\n",
      "                            np.int64(69): np.int64(6),\n",
      "                            np.int64(70): np.int64(2),\n",
      "                            np.int64(71): np.int64(260),\n",
      "                            np.int64(72): np.int64(9),\n",
      "                            np.int64(73): np.int64(1),\n",
      "                            np.int64(74): np.int64(30),\n",
      "                            np.int64(75): np.int64(10),\n",
      "                            np.int64(76): np.int64(58),\n",
      "                            np.int64(77): np.int64(19),\n",
      "                            np.int64(78): np.int64(15),\n",
      "                            np.int64(79): np.int64(2),\n",
      "                            np.int64(80): np.int64(4),\n",
      "                            np.int64(81): np.int64(1),\n",
      "                            np.int64(83): np.int64(1),\n",
      "                            np.int64(84): np.int64(6),\n",
      "                            np.int64(85): np.int64(50),\n",
      "                            np.int64(86): np.int64(9),\n",
      "                            np.int64(87): np.int64(140),\n",
      "                            np.int64(89): np.int64(19),\n",
      "                            np.int64(90): np.int64(25),\n",
      "                            np.int64(91): np.int64(131),\n",
      "                            np.int64(92): np.int64(121),\n",
      "                            np.int64(93): np.int64(136),\n",
      "                            np.int64(94): np.int64(93),\n",
      "                            np.int64(95): np.int64(54),\n",
      "                            np.int64(96): np.int64(5),\n",
      "                            np.int64(97): np.int64(105),\n",
      "                            np.int64(98): np.int64(51),\n",
      "                            np.int64(99): np.int64(17)},\n",
      "     'gradient_norm': 4.68546199798584,\n",
      "     'learning_rate': 0.01,\n",
      "     'local_epochs': 1,\n",
      "     'local_loss': 2.6069793701171875,\n",
      "     'num_samples': 5014,\n",
      "     'per_layer_norms': {'bn1.bias': 0.06122458353638649,\n",
      "                         'bn1.weight': 0.12056384980678558,\n",
      "                         'conv1.weight': 1.169519066810608,\n",
      "                         'fc.bias': 0.12527622282505035,\n",
      "                         'fc.weight': 2.284205675125122,\n",
      "                         'layer1.0.bn1.bias': 0.05765624716877937,\n",
      "                         'layer1.0.bn1.weight': 0.07050875574350357,\n",
      "                         'layer1.0.bn2.bias': 0.04130964353680611,\n",
      "                         'layer1.0.bn2.weight': 0.053169600665569305,\n",
      "                         'layer1.0.conv1.weight': 0.8297752737998962,\n",
      "                         'layer1.0.conv2.weight': 0.7891071438789368,\n",
      "                         'layer1.1.bn1.bias': 0.04189223423600197,\n",
      "                         'layer1.1.bn1.weight': 0.04579220712184906,\n",
      "                         'layer1.1.bn2.bias': 0.02848442643880844,\n",
      "                         'layer1.1.bn2.weight': 0.05116315931081772,\n",
      "                         'layer1.1.conv1.weight': 0.6790415644645691,\n",
      "                         'layer1.1.conv2.weight': 0.6665326356887817,\n",
      "                         'layer2.0.bn1.bias': 0.040760498493909836,\n",
      "                         'layer2.0.bn1.weight': 0.04887857288122177,\n",
      "                         'layer2.0.bn2.bias': 0.04123277962207794,\n",
      "                         'layer2.0.bn2.weight': 0.05217335373163223,\n",
      "                         'layer2.0.conv1.weight': 0.945111095905304,\n",
      "                         'layer2.0.conv2.weight': 1.0147844552993774,\n",
      "                         'layer2.0.downsample.0.weight': 0.3938523828983307,\n",
      "                         'layer2.0.downsample.1.bias': 0.04123277962207794,\n",
      "                         'layer2.0.downsample.1.weight': 0.04506083205342293,\n",
      "                         'layer2.1.bn1.bias': 0.03352944180369377,\n",
      "                         'layer2.1.bn1.weight': 0.04130460321903229,\n",
      "                         'layer2.1.bn2.bias': 0.026334134861826897,\n",
      "                         'layer2.1.bn2.weight': 0.040739256888628006,\n",
      "                         'layer2.1.conv1.weight': 0.9809920191764832,\n",
      "                         'layer2.1.conv2.weight': 0.8842833638191223,\n",
      "                         'layer3.0.bn1.bias': 0.03533828631043434,\n",
      "                         'layer3.0.bn1.weight': 0.04443016275763512,\n",
      "                         'layer3.0.bn2.bias': 0.032342396676540375,\n",
      "                         'layer3.0.bn2.weight': 0.04560807719826698,\n",
      "                         'layer3.0.conv1.weight': 1.228437066078186,\n",
      "                         'layer3.0.conv2.weight': 1.1377888917922974,\n",
      "                         'layer3.0.downsample.0.weight': 0.43390610814094543,\n",
      "                         'layer3.0.downsample.1.bias': 0.032342396676540375,\n",
      "                         'layer3.0.downsample.1.weight': 0.03682925924658775,\n",
      "                         'layer3.1.bn1.bias': 0.026604514569044113,\n",
      "                         'layer3.1.bn1.weight': 0.026731474325060844,\n",
      "                         'layer3.1.bn2.bias': 0.01986384391784668,\n",
      "                         'layer3.1.bn2.weight': 0.03166468068957329,\n",
      "                         'layer3.1.conv1.weight': 0.9152237176895142,\n",
      "                         'layer3.1.conv2.weight': 0.8116621375083923,\n",
      "                         'layer4.0.bn1.bias': 0.025497809052467346,\n",
      "                         'layer4.0.bn1.weight': 0.029163574799895287,\n",
      "                         'layer4.0.bn2.bias': 0.05898295342922211,\n",
      "                         'layer4.0.bn2.weight': 0.04817568138241768,\n",
      "                         'layer4.0.conv1.weight': 1.0820280313491821,\n",
      "                         'layer4.0.conv2.weight': 1.0926839113235474,\n",
      "                         'layer4.0.downsample.0.weight': 0.6749950051307678,\n",
      "                         'layer4.0.downsample.1.bias': 0.05898295342922211,\n",
      "                         'layer4.0.downsample.1.weight': 0.05460980907082558,\n",
      "                         'layer4.1.bn1.bias': 0.026195529848337173,\n",
      "                         'layer4.1.bn1.weight': 0.028016384690999985,\n",
      "                         'layer4.1.bn2.bias': 0.07244323939085007,\n",
      "                         'layer4.1.bn2.weight': 0.04420824348926544,\n",
      "                         'layer4.1.conv1.weight': 0.9854997396469116,\n",
      "                         'layer4.1.conv2.weight': 0.9714853763580322}},\n",
      " 2: {'batch_size': 64,\n",
      "     'class_distribution': {np.int64(0): np.int64(77),\n",
      "                            np.int64(1): np.int64(40),\n",
      "                            np.int64(2): np.int64(5),\n",
      "                            np.int64(3): np.int64(154),\n",
      "                            np.int64(4): np.int64(129),\n",
      "                            np.int64(5): np.int64(80),\n",
      "                            np.int64(6): np.int64(40),\n",
      "                            np.int64(7): np.int64(1),\n",
      "                            np.int64(8): np.int64(49),\n",
      "                            np.int64(9): np.int64(207),\n",
      "                            np.int64(10): np.int64(20),\n",
      "                            np.int64(11): np.int64(97),\n",
      "                            np.int64(12): np.int64(10),\n",
      "                            np.int64(13): np.int64(14),\n",
      "                            np.int64(14): np.int64(43),\n",
      "                            np.int64(15): np.int64(40),\n",
      "                            np.int64(16): np.int64(2),\n",
      "                            np.int64(17): np.int64(13),\n",
      "                            np.int64(18): np.int64(7),\n",
      "                            np.int64(19): np.int64(4),\n",
      "                            np.int64(20): np.int64(3),\n",
      "                            np.int64(21): np.int64(14),\n",
      "                            np.int64(22): np.int64(48),\n",
      "                            np.int64(23): np.int64(136),\n",
      "                            np.int64(24): np.int64(43),\n",
      "                            np.int64(25): np.int64(4),\n",
      "                            np.int64(26): np.int64(13),\n",
      "                            np.int64(27): np.int64(9),\n",
      "                            np.int64(28): np.int64(4),\n",
      "                            np.int64(29): np.int64(142),\n",
      "                            np.int64(30): np.int64(2),\n",
      "                            np.int64(31): np.int64(96),\n",
      "                            np.int64(32): np.int64(13),\n",
      "                            np.int64(33): np.int64(6),\n",
      "                            np.int64(34): np.int64(76),\n",
      "                            np.int64(35): np.int64(4),\n",
      "                            np.int64(36): np.int64(1),\n",
      "                            np.int64(37): np.int64(91),\n",
      "                            np.int64(38): np.int64(40),\n",
      "                            np.int64(39): np.int64(246),\n",
      "                            np.int64(40): np.int64(61),\n",
      "                            np.int64(41): np.int64(212),\n",
      "                            np.int64(42): np.int64(62),\n",
      "                            np.int64(43): np.int64(248),\n",
      "                            np.int64(45): np.int64(30),\n",
      "                            np.int64(46): np.int64(249),\n",
      "                            np.int64(47): np.int64(116),\n",
      "                            np.int64(48): np.int64(190),\n",
      "                            np.int64(49): np.int64(119),\n",
      "                            np.int64(52): np.int64(8),\n",
      "                            np.int64(53): np.int64(1),\n",
      "                            np.int64(55): np.int64(133),\n",
      "                            np.int64(56): np.int64(28),\n",
      "                            np.int64(57): np.int64(42),\n",
      "                            np.int64(58): np.int64(13),\n",
      "                            np.int64(59): np.int64(2),\n",
      "                            np.int64(60): np.int64(31),\n",
      "                            np.int64(61): np.int64(104),\n",
      "                            np.int64(62): np.int64(37),\n",
      "                            np.int64(63): np.int64(11),\n",
      "                            np.int64(64): np.int64(2),\n",
      "                            np.int64(65): np.int64(59),\n",
      "                            np.int64(66): np.int64(32),\n",
      "                            np.int64(67): np.int64(20),\n",
      "                            np.int64(68): np.int64(11),\n",
      "                            np.int64(69): np.int64(2),\n",
      "                            np.int64(70): np.int64(92),\n",
      "                            np.int64(71): np.int64(50),\n",
      "                            np.int64(72): np.int64(17),\n",
      "                            np.int64(73): np.int64(22),\n",
      "                            np.int64(74): np.int64(134),\n",
      "                            np.int64(75): np.int64(188),\n",
      "                            np.int64(76): np.int64(27),\n",
      "                            np.int64(77): np.int64(3),\n",
      "                            np.int64(78): np.int64(29),\n",
      "                            np.int64(79): np.int64(31),\n",
      "                            np.int64(80): np.int64(6),\n",
      "                            np.int64(81): np.int64(8),\n",
      "                            np.int64(82): np.int64(30),\n",
      "                            np.int64(83): np.int64(147),\n",
      "                            np.int64(84): np.int64(9),\n",
      "                            np.int64(85): np.int64(105),\n",
      "                            np.int64(86): np.int64(2),\n",
      "                            np.int64(87): np.int64(4),\n",
      "                            np.int64(88): np.int64(166),\n",
      "                            np.int64(89): np.int64(18),\n",
      "                            np.int64(90): np.int64(272),\n",
      "                            np.int64(91): np.int64(106),\n",
      "                            np.int64(92): np.int64(11),\n",
      "                            np.int64(93): np.int64(14),\n",
      "                            np.int64(94): np.int64(5),\n",
      "                            np.int64(95): np.int64(15),\n",
      "                            np.int64(96): np.int64(26),\n",
      "                            np.int64(97): np.int64(168),\n",
      "                            np.int64(98): np.int64(31)},\n",
      "     'gradient_norm': 4.972283840179443,\n",
      "     'learning_rate': 0.01,\n",
      "     'local_epochs': 1,\n",
      "     'local_loss': 2.457716464996338,\n",
      "     'num_samples': 5582,\n",
      "     'per_layer_norms': {'bn1.bias': 0.0552186593413353,\n",
      "                         'bn1.weight': 0.09924472868442535,\n",
      "                         'conv1.weight': 1.2443177700042725,\n",
      "                         'fc.bias': 0.1548020839691162,\n",
      "                         'fc.weight': 2.618150472640991,\n",
      "                         'layer1.0.bn1.bias': 0.061265699565410614,\n",
      "                         'layer1.0.bn1.weight': 0.06101193279027939,\n",
      "                         'layer1.0.bn2.bias': 0.03716258704662323,\n",
      "                         'layer1.0.bn2.weight': 0.05952839180827141,\n",
      "                         'layer1.0.conv1.weight': 0.8196353316307068,\n",
      "                         'layer1.0.conv2.weight': 0.799463152885437,\n",
      "                         'layer1.1.bn1.bias': 0.03725506365299225,\n",
      "                         'layer1.1.bn1.weight': 0.045896586030721664,\n",
      "                         'layer1.1.bn2.bias': 0.028443844988942146,\n",
      "                         'layer1.1.bn2.weight': 0.04850166663527489,\n",
      "                         'layer1.1.conv1.weight': 0.7073955535888672,\n",
      "                         'layer1.1.conv2.weight': 0.6884835958480835,\n",
      "                         'layer2.0.bn1.bias': 0.0424201600253582,\n",
      "                         'layer2.0.bn1.weight': 0.05130323767662048,\n",
      "                         'layer2.0.bn2.bias': 0.039250634610652924,\n",
      "                         'layer2.0.bn2.weight': 0.052074432373046875,\n",
      "                         'layer2.0.conv1.weight': 0.9430856108665466,\n",
      "                         'layer2.0.conv2.weight': 1.032832145690918,\n",
      "                         'layer2.0.downsample.0.weight': 0.4096313714981079,\n",
      "                         'layer2.0.downsample.1.bias': 0.039250634610652924,\n",
      "                         'layer2.0.downsample.1.weight': 0.04849967360496521,\n",
      "                         'layer2.1.bn1.bias': 0.03771061822772026,\n",
      "                         'layer2.1.bn1.weight': 0.045263148844242096,\n",
      "                         'layer2.1.bn2.bias': 0.02958524413406849,\n",
      "                         'layer2.1.bn2.weight': 0.03846101462841034,\n",
      "                         'layer2.1.conv1.weight': 0.9902667999267578,\n",
      "                         'layer2.1.conv2.weight': 0.9117642641067505,\n",
      "                         'layer3.0.bn1.bias': 0.03504559397697449,\n",
      "                         'layer3.0.bn1.weight': 0.04008922725915909,\n",
      "                         'layer3.0.bn2.bias': 0.0338611900806427,\n",
      "                         'layer3.0.bn2.weight': 0.041838373988866806,\n",
      "                         'layer3.0.conv1.weight': 1.263919472694397,\n",
      "                         'layer3.0.conv2.weight': 1.1817312240600586,\n",
      "                         'layer3.0.downsample.0.weight': 0.4515751898288727,\n",
      "                         'layer3.0.downsample.1.bias': 0.0338611900806427,\n",
      "                         'layer3.0.downsample.1.weight': 0.03606594353914261,\n",
      "                         'layer3.1.bn1.bias': 0.026818659156560898,\n",
      "                         'layer3.1.bn1.weight': 0.028781915083527565,\n",
      "                         'layer3.1.bn2.bias': 0.021380674093961716,\n",
      "                         'layer3.1.bn2.weight': 0.03175446391105652,\n",
      "                         'layer3.1.conv1.weight': 0.9467638731002808,\n",
      "                         'layer3.1.conv2.weight': 0.8494909405708313,\n",
      "                         'layer4.0.bn1.bias': 0.02620680071413517,\n",
      "                         'layer4.0.bn1.weight': 0.03187543898820877,\n",
      "                         'layer4.0.bn2.bias': 0.06088767573237419,\n",
      "                         'layer4.0.bn2.weight': 0.05564442276954651,\n",
      "                         'layer4.0.conv1.weight': 1.1465623378753662,\n",
      "                         'layer4.0.conv2.weight': 1.1502759456634521,\n",
      "                         'layer4.0.downsample.0.weight': 0.7152600884437561,\n",
      "                         'layer4.0.downsample.1.bias': 0.06088767573237419,\n",
      "                         'layer4.0.downsample.1.weight': 0.06306730955839157,\n",
      "                         'layer4.1.bn1.bias': 0.023450834676623344,\n",
      "                         'layer4.1.bn1.weight': 0.028066352009773254,\n",
      "                         'layer4.1.bn2.bias': 0.07637063413858414,\n",
      "                         'layer4.1.bn2.weight': 0.052913933992385864,\n",
      "                         'layer4.1.conv1.weight': 0.9979654550552368,\n",
      "                         'layer4.1.conv2.weight': 1.0131498575210571}},\n",
      " 3: {'batch_size': 64,\n",
      "     'class_distribution': {np.int64(0): np.int64(40),\n",
      "                            np.int64(1): np.int64(96),\n",
      "                            np.int64(3): np.int64(21),\n",
      "                            np.int64(4): np.int64(1),\n",
      "                            np.int64(5): np.int64(136),\n",
      "                            np.int64(6): np.int64(52),\n",
      "                            np.int64(7): np.int64(38),\n",
      "                            np.int64(8): np.int64(5),\n",
      "                            np.int64(9): np.int64(3),\n",
      "                            np.int64(10): np.int64(2),\n",
      "                            np.int64(11): np.int64(26),\n",
      "                            np.int64(12): np.int64(7),\n",
      "                            np.int64(13): np.int64(104),\n",
      "                            np.int64(14): np.int64(44),\n",
      "                            np.int64(15): np.int64(10),\n",
      "                            np.int64(16): np.int64(37),\n",
      "                            np.int64(17): np.int64(37),\n",
      "                            np.int64(18): np.int64(6),\n",
      "                            np.int64(19): np.int64(25),\n",
      "                            np.int64(20): np.int64(3),\n",
      "                            np.int64(21): np.int64(135),\n",
      "                            np.int64(22): np.int64(1),\n",
      "                            np.int64(23): np.int64(8),\n",
      "                            np.int64(24): np.int64(3),\n",
      "                            np.int64(25): np.int64(65),\n",
      "                            np.int64(26): np.int64(249),\n",
      "                            np.int64(27): np.int64(56),\n",
      "                            np.int64(28): np.int64(10),\n",
      "                            np.int64(29): np.int64(1),\n",
      "                            np.int64(30): np.int64(303),\n",
      "                            np.int64(31): np.int64(3),\n",
      "                            np.int64(32): np.int64(1),\n",
      "                            np.int64(33): np.int64(20),\n",
      "                            np.int64(34): np.int64(166),\n",
      "                            np.int64(35): np.int64(22),\n",
      "                            np.int64(36): np.int64(6),\n",
      "                            np.int64(37): np.int64(39),\n",
      "                            np.int64(38): np.int64(56),\n",
      "                            np.int64(39): np.int64(76),\n",
      "                            np.int64(40): np.int64(13),\n",
      "                            np.int64(41): np.int64(3),\n",
      "                            np.int64(42): np.int64(17),\n",
      "                            np.int64(43): np.int64(19),\n",
      "                            np.int64(44): np.int64(2),\n",
      "                            np.int64(45): np.int64(45),\n",
      "                            np.int64(46): np.int64(11),\n",
      "                            np.int64(47): np.int64(97),\n",
      "                            np.int64(48): np.int64(9),\n",
      "                            np.int64(49): np.int64(14),\n",
      "                            np.int64(50): np.int64(18),\n",
      "                            np.int64(51): np.int64(11),\n",
      "                            np.int64(52): np.int64(104),\n",
      "                            np.int64(53): np.int64(48),\n",
      "                            np.int64(54): np.int64(138),\n",
      "                            np.int64(55): np.int64(18),\n",
      "                            np.int64(56): np.int64(10),\n",
      "                            np.int64(57): np.int64(68),\n",
      "                            np.int64(58): np.int64(16),\n",
      "                            np.int64(59): np.int64(8),\n",
      "                            np.int64(60): np.int64(35),\n",
      "                            np.int64(61): np.int64(132),\n",
      "                            np.int64(62): np.int64(18),\n",
      "                            np.int64(63): np.int64(11),\n",
      "                            np.int64(65): np.int64(23),\n",
      "                            np.int64(66): np.int64(11),\n",
      "                            np.int64(67): np.int64(35),\n",
      "                            np.int64(68): np.int64(15),\n",
      "                            np.int64(69): np.int64(9),\n",
      "                            np.int64(70): np.int64(21),\n",
      "                            np.int64(71): np.int64(5),\n",
      "                            np.int64(72): np.int64(3),\n",
      "                            np.int64(73): np.int64(10),\n",
      "                            np.int64(74): np.int64(4),\n",
      "                            np.int64(75): np.int64(14),\n",
      "                            np.int64(76): np.int64(90),\n",
      "                            np.int64(77): np.int64(178),\n",
      "                            np.int64(79): np.int64(38),\n",
      "                            np.int64(80): np.int64(162),\n",
      "                            np.int64(81): np.int64(13),\n",
      "                            np.int64(82): np.int64(53),\n",
      "                            np.int64(83): np.int64(9),\n",
      "                            np.int64(84): np.int64(154),\n",
      "                            np.int64(85): np.int64(6),\n",
      "                            np.int64(86): np.int64(144),\n",
      "                            np.int64(87): np.int64(83),\n",
      "                            np.int64(88): np.int64(18),\n",
      "                            np.int64(89): np.int64(46),\n",
      "                            np.int64(90): np.int64(27),\n",
      "                            np.int64(91): np.int64(85),\n",
      "                            np.int64(92): np.int64(2),\n",
      "                            np.int64(93): np.int64(92),\n",
      "                            np.int64(94): np.int64(81),\n",
      "                            np.int64(95): np.int64(59),\n",
      "                            np.int64(96): np.int64(17),\n",
      "                            np.int64(97): np.int64(14)},\n",
      "     'gradient_norm': 5.405187606811523,\n",
      "     'learning_rate': 0.01,\n",
      "     'local_epochs': 1,\n",
      "     'local_loss': 2.863546371459961,\n",
      "     'num_samples': 4299,\n",
      "     'per_layer_norms': {'bn1.bias': 0.053155455738306046,\n",
      "                         'bn1.weight': 0.12580248713493347,\n",
      "                         'conv1.weight': 1.4312726259231567,\n",
      "                         'fc.bias': 0.1517597883939743,\n",
      "                         'fc.weight': 2.7746002674102783,\n",
      "                         'layer1.0.bn1.bias': 0.0656774714589119,\n",
      "                         'layer1.0.bn1.weight': 0.06226707994937897,\n",
      "                         'layer1.0.bn2.bias': 0.040378205478191376,\n",
      "                         'layer1.0.bn2.weight': 0.05896705761551857,\n",
      "                         'layer1.0.conv1.weight': 0.9259626269340515,\n",
      "                         'layer1.0.conv2.weight': 0.8606507778167725,\n",
      "                         'layer1.1.bn1.bias': 0.03911823406815529,\n",
      "                         'layer1.1.bn1.weight': 0.04872418940067291,\n",
      "                         'layer1.1.bn2.bias': 0.028930777683854103,\n",
      "                         'layer1.1.bn2.weight': 0.05237492918968201,\n",
      "                         'layer1.1.conv1.weight': 0.729677140712738,\n",
      "                         'layer1.1.conv2.weight': 0.7420140504837036,\n",
      "                         'layer2.0.bn1.bias': 0.04160875082015991,\n",
      "                         'layer2.0.bn1.weight': 0.0477805957198143,\n",
      "                         'layer2.0.bn2.bias': 0.04773327708244324,\n",
      "                         'layer2.0.bn2.weight': 0.06046958640217781,\n",
      "                         'layer2.0.conv1.weight': 1.0584007501602173,\n",
      "                         'layer2.0.conv2.weight': 1.1348450183868408,\n",
      "                         'layer2.0.downsample.0.weight': 0.44712021946907043,\n",
      "                         'layer2.0.downsample.1.bias': 0.04773327708244324,\n",
      "                         'layer2.0.downsample.1.weight': 0.05832741782069206,\n",
      "                         'layer2.1.bn1.bias': 0.04138756915926933,\n",
      "                         'layer2.1.bn1.weight': 0.05208702012896538,\n",
      "                         'layer2.1.bn2.bias': 0.027241617441177368,\n",
      "                         'layer2.1.bn2.weight': 0.0374683141708374,\n",
      "                         'layer2.1.conv1.weight': 1.081068992614746,\n",
      "                         'layer2.1.conv2.weight': 0.9995766878128052,\n",
      "                         'layer3.0.bn1.bias': 0.034749288111925125,\n",
      "                         'layer3.0.bn1.weight': 0.04305892437696457,\n",
      "                         'layer3.0.bn2.bias': 0.03164288401603699,\n",
      "                         'layer3.0.bn2.weight': 0.05275406688451767,\n",
      "                         'layer3.0.conv1.weight': 1.3813574314117432,\n",
      "                         'layer3.0.conv2.weight': 1.2697335481643677,\n",
      "                         'layer3.0.downsample.0.weight': 0.49083957076072693,\n",
      "                         'layer3.0.downsample.1.bias': 0.03164288401603699,\n",
      "                         'layer3.0.downsample.1.weight': 0.043791115283966064,\n",
      "                         'layer3.1.bn1.bias': 0.023168880492448807,\n",
      "                         'layer3.1.bn1.weight': 0.026281282305717468,\n",
      "                         'layer3.1.bn2.bias': 0.021415116265416145,\n",
      "                         'layer3.1.bn2.weight': 0.03077997826039791,\n",
      "                         'layer3.1.conv1.weight': 1.0453943014144897,\n",
      "                         'layer3.1.conv2.weight': 0.9236868619918823,\n",
      "                         'layer4.0.bn1.bias': 0.029114676639437675,\n",
      "                         'layer4.0.bn1.weight': 0.032668206840753555,\n",
      "                         'layer4.0.bn2.bias': 0.06510748714208603,\n",
      "                         'layer4.0.bn2.weight': 0.06003886088728905,\n",
      "                         'layer4.0.conv1.weight': 1.275386929512024,\n",
      "                         'layer4.0.conv2.weight': 1.2795668840408325,\n",
      "                         'layer4.0.downsample.0.weight': 0.7356045842170715,\n",
      "                         'layer4.0.downsample.1.bias': 0.06510748714208603,\n",
      "                         'layer4.0.downsample.1.weight': 0.06272989511489868,\n",
      "                         'layer4.1.bn1.bias': 0.026792168617248535,\n",
      "                         'layer4.1.bn1.weight': 0.0289804395288229,\n",
      "                         'layer4.1.bn2.bias': 0.07561572641134262,\n",
      "                         'layer4.1.bn2.weight': 0.04979389160871506,\n",
      "                         'layer4.1.conv1.weight': 1.0925902128219604,\n",
      "                         'layer4.1.conv2.weight': 1.0868475437164307}},\n",
      " 4: {'batch_size': 64,\n",
      "     'class_distribution': {np.int64(0): np.int64(208),\n",
      "                            np.int64(1): np.int64(16),\n",
      "                            np.int64(2): np.int64(263),\n",
      "                            np.int64(3): np.int64(4),\n",
      "                            np.int64(4): np.int64(3),\n",
      "                            np.int64(7): np.int64(117),\n",
      "                            np.int64(8): np.int64(24),\n",
      "                            np.int64(9): np.int64(16),\n",
      "                            np.int64(10): np.int64(117),\n",
      "                            np.int64(11): np.int64(61),\n",
      "                            np.int64(12): np.int64(4),\n",
      "                            np.int64(13): np.int64(1),\n",
      "                            np.int64(14): np.int64(42),\n",
      "                            np.int64(15): np.int64(41),\n",
      "                            np.int64(16): np.int64(116),\n",
      "                            np.int64(17): np.int64(105),\n",
      "                            np.int64(18): np.int64(17),\n",
      "                            np.int64(19): np.int64(25),\n",
      "                            np.int64(20): np.int64(99),\n",
      "                            np.int64(21): np.int64(1),\n",
      "                            np.int64(22): np.int64(151),\n",
      "                            np.int64(23): np.int64(24),\n",
      "                            np.int64(25): np.int64(24),\n",
      "                            np.int64(26): np.int64(10),\n",
      "                            np.int64(27): np.int64(49),\n",
      "                            np.int64(28): np.int64(45),\n",
      "                            np.int64(29): np.int64(77),\n",
      "                            np.int64(30): np.int64(38),\n",
      "                            np.int64(31): np.int64(1),\n",
      "                            np.int64(32): np.int64(32),\n",
      "                            np.int64(33): np.int64(119),\n",
      "                            np.int64(34): np.int64(28),\n",
      "                            np.int64(35): np.int64(104),\n",
      "                            np.int64(36): np.int64(78),\n",
      "                            np.int64(37): np.int64(159),\n",
      "                            np.int64(38): np.int64(24),\n",
      "                            np.int64(39): np.int64(17),\n",
      "                            np.int64(40): np.int64(2),\n",
      "                            np.int64(42): np.int64(157),\n",
      "                            np.int64(43): np.int64(34),\n",
      "                            np.int64(44): np.int64(27),\n",
      "                            np.int64(45): np.int64(91),\n",
      "                            np.int64(46): np.int64(44),\n",
      "                            np.int64(47): np.int64(97),\n",
      "                            np.int64(48): np.int64(1),\n",
      "                            np.int64(49): np.int64(84),\n",
      "                            np.int64(50): np.int64(4),\n",
      "                            np.int64(51): np.int64(33),\n",
      "                            np.int64(52): np.int64(80),\n",
      "                            np.int64(53): np.int64(4),\n",
      "                            np.int64(54): np.int64(94),\n",
      "                            np.int64(55): np.int64(35),\n",
      "                            np.int64(56): np.int64(26),\n",
      "                            np.int64(57): np.int64(103),\n",
      "                            np.int64(58): np.int64(1),\n",
      "                            np.int64(59): np.int64(51),\n",
      "                            np.int64(60): np.int64(21),\n",
      "                            np.int64(62): np.int64(148),\n",
      "                            np.int64(63): np.int64(7),\n",
      "                            np.int64(64): np.int64(4),\n",
      "                            np.int64(65): np.int64(61),\n",
      "                            np.int64(66): np.int64(44),\n",
      "                            np.int64(67): np.int64(21),\n",
      "                            np.int64(68): np.int64(24),\n",
      "                            np.int64(69): np.int64(46),\n",
      "                            np.int64(70): np.int64(4),\n",
      "                            np.int64(71): np.int64(20),\n",
      "                            np.int64(72): np.int64(18),\n",
      "                            np.int64(73): np.int64(19),\n",
      "                            np.int64(74): np.int64(34),\n",
      "                            np.int64(75): np.int64(10),\n",
      "                            np.int64(76): np.int64(15),\n",
      "                            np.int64(77): np.int64(3),\n",
      "                            np.int64(78): np.int64(4),\n",
      "                            np.int64(79): np.int64(7),\n",
      "                            np.int64(80): np.int64(2),\n",
      "                            np.int64(81): np.int64(237),\n",
      "                            np.int64(82): np.int64(8),\n",
      "                            np.int64(83): np.int64(54),\n",
      "                            np.int64(84): np.int64(20),\n",
      "                            np.int64(85): np.int64(12),\n",
      "                            np.int64(86): np.int64(1),\n",
      "                            np.int64(87): np.int64(31),\n",
      "                            np.int64(88): np.int64(58),\n",
      "                            np.int64(89): np.int64(102),\n",
      "                            np.int64(90): np.int64(3),\n",
      "                            np.int64(91): np.int64(60),\n",
      "                            np.int64(92): np.int64(206),\n",
      "                            np.int64(93): np.int64(21),\n",
      "                            np.int64(94): np.int64(9),\n",
      "                            np.int64(96): np.int64(5),\n",
      "                            np.int64(97): np.int64(4),\n",
      "                            np.int64(98): np.int64(1),\n",
      "                            np.int64(99): np.int64(68)},\n",
      "     'gradient_norm': 4.924473762512207,\n",
      "     'learning_rate': 0.01,\n",
      "     'local_epochs': 1,\n",
      "     'local_loss': 3.136077880859375,\n",
      "     'num_samples': 4640,\n",
      "     'per_layer_norms': {'bn1.bias': 0.06778769195079803,\n",
      "                         'bn1.weight': 0.10088635236024857,\n",
      "                         'conv1.weight': 1.0570460557937622,\n",
      "                         'fc.bias': 0.1436874121427536,\n",
      "                         'fc.weight': 2.455744504928589,\n",
      "                         'layer1.0.bn1.bias': 0.054258864372968674,\n",
      "                         'layer1.0.bn1.weight': 0.05249694362282753,\n",
      "                         'layer1.0.bn2.bias': 0.03976103290915489,\n",
      "                         'layer1.0.bn2.weight': 0.0583248995244503,\n",
      "                         'layer1.0.conv1.weight': 0.8754477500915527,\n",
      "                         'layer1.0.conv2.weight': 0.8008359670639038,\n",
      "                         'layer1.1.bn1.bias': 0.04066190868616104,\n",
      "                         'layer1.1.bn1.weight': 0.039805978536605835,\n",
      "                         'layer1.1.bn2.bias': 0.02997848577797413,\n",
      "                         'layer1.1.bn2.weight': 0.04111964628100395,\n",
      "                         'layer1.1.conv1.weight': 0.6971257328987122,\n",
      "                         'layer1.1.conv2.weight': 0.681850790977478,\n",
      "                         'layer2.0.bn1.bias': 0.04456145316362381,\n",
      "                         'layer2.0.bn1.weight': 0.05162394046783447,\n",
      "                         'layer2.0.bn2.bias': 0.042083658277988434,\n",
      "                         'layer2.0.bn2.weight': 0.05383515730500221,\n",
      "                         'layer2.0.conv1.weight': 1.0066310167312622,\n",
      "                         'layer2.0.conv2.weight': 1.0810214281082153,\n",
      "                         'layer2.0.downsample.0.weight': 0.42571884393692017,\n",
      "                         'layer2.0.downsample.1.bias': 0.042083658277988434,\n",
      "                         'layer2.0.downsample.1.weight': 0.04723910614848137,\n",
      "                         'layer2.1.bn1.bias': 0.039311185479164124,\n",
      "                         'layer2.1.bn1.weight': 0.040362052619457245,\n",
      "                         'layer2.1.bn2.bias': 0.031077861785888672,\n",
      "                         'layer2.1.bn2.weight': 0.037485938519239426,\n",
      "                         'layer2.1.conv1.weight': 1.048705816268921,\n",
      "                         'layer2.1.conv2.weight': 0.955290675163269,\n",
      "                         'layer3.0.bn1.bias': 0.037289008498191833,\n",
      "                         'layer3.0.bn1.weight': 0.04536280781030655,\n",
      "                         'layer3.0.bn2.bias': 0.03291166573762894,\n",
      "                         'layer3.0.bn2.weight': 0.044047534465789795,\n",
      "                         'layer3.0.conv1.weight': 1.3248741626739502,\n",
      "                         'layer3.0.conv2.weight': 1.225933313369751,\n",
      "                         'layer3.0.downsample.0.weight': 0.46703121066093445,\n",
      "                         'layer3.0.downsample.1.bias': 0.03291166573762894,\n",
      "                         'layer3.0.downsample.1.weight': 0.0385512113571167,\n",
      "                         'layer3.1.bn1.bias': 0.030496761202812195,\n",
      "                         'layer3.1.bn1.weight': 0.029156532138586044,\n",
      "                         'layer3.1.bn2.bias': 0.021045202389359474,\n",
      "                         'layer3.1.bn2.weight': 0.031032254919409752,\n",
      "                         'layer3.1.conv1.weight': 0.9772182703018188,\n",
      "                         'layer3.1.conv2.weight': 0.8725865483283997,\n",
      "                         'layer4.0.bn1.bias': 0.02741927281022072,\n",
      "                         'layer4.0.bn1.weight': 0.03136281669139862,\n",
      "                         'layer4.0.bn2.bias': 0.06223028898239136,\n",
      "                         'layer4.0.bn2.weight': 0.05000143125653267,\n",
      "                         'layer4.0.conv1.weight': 1.1601563692092896,\n",
      "                         'layer4.0.conv2.weight': 1.1553444862365723,\n",
      "                         'layer4.0.downsample.0.weight': 0.7065389156341553,\n",
      "                         'layer4.0.downsample.1.bias': 0.06223028898239136,\n",
      "                         'layer4.0.downsample.1.weight': 0.05437817424535751,\n",
      "                         'layer4.1.bn1.bias': 0.025025993585586548,\n",
      "                         'layer4.1.bn1.weight': 0.02565239556133747,\n",
      "                         'layer4.1.bn2.bias': 0.07614731788635254,\n",
      "                         'layer4.1.bn2.weight': 0.04426704719662666,\n",
      "                         'layer4.1.conv1.weight': 0.9880673885345459,\n",
      "                         'layer4.1.conv2.weight': 0.9551473259925842}},\n",
      " 5: {'batch_size': 64,\n",
      "     'class_distribution': {np.int64(0): np.int64(21),\n",
      "                            np.int64(1): np.int64(74),\n",
      "                            np.int64(2): np.int64(20),\n",
      "                            np.int64(3): np.int64(20),\n",
      "                            np.int64(4): np.int64(15),\n",
      "                            np.int64(5): np.int64(26),\n",
      "                            np.int64(6): np.int64(54),\n",
      "                            np.int64(7): np.int64(23),\n",
      "                            np.int64(8): np.int64(14),\n",
      "                            np.int64(9): np.int64(14),\n",
      "                            np.int64(10): np.int64(109),\n",
      "                            np.int64(11): np.int64(10),\n",
      "                            np.int64(12): np.int64(12),\n",
      "                            np.int64(13): np.int64(14),\n",
      "                            np.int64(14): np.int64(118),\n",
      "                            np.int64(15): np.int64(222),\n",
      "                            np.int64(16): np.int64(3),\n",
      "                            np.int64(17): np.int64(10),\n",
      "                            np.int64(18): np.int64(276),\n",
      "                            np.int64(19): np.int64(1),\n",
      "                            np.int64(20): np.int64(19),\n",
      "                            np.int64(21): np.int64(83),\n",
      "                            np.int64(22): np.int64(59),\n",
      "                            np.int64(23): np.int64(3),\n",
      "                            np.int64(25): np.int64(97),\n",
      "                            np.int64(26): np.int64(69),\n",
      "                            np.int64(27): np.int64(139),\n",
      "                            np.int64(28): np.int64(66),\n",
      "                            np.int64(29): np.int64(183),\n",
      "                            np.int64(30): np.int64(43),\n",
      "                            np.int64(32): np.int64(244),\n",
      "                            np.int64(33): np.int64(17),\n",
      "                            np.int64(34): np.int64(109),\n",
      "                            np.int64(35): np.int64(4),\n",
      "                            np.int64(36): np.int64(80),\n",
      "                            np.int64(37): np.int64(12),\n",
      "                            np.int64(38): np.int64(29),\n",
      "                            np.int64(39): np.int64(38),\n",
      "                            np.int64(40): np.int64(262),\n",
      "                            np.int64(41): np.int64(74),\n",
      "                            np.int64(42): np.int64(79),\n",
      "                            np.int64(43): np.int64(7),\n",
      "                            np.int64(44): np.int64(47),\n",
      "                            np.int64(45): np.int64(2),\n",
      "                            np.int64(46): np.int64(5),\n",
      "                            np.int64(47): np.int64(29),\n",
      "                            np.int64(48): np.int64(13),\n",
      "                            np.int64(49): np.int64(46),\n",
      "                            np.int64(50): np.int64(213),\n",
      "                            np.int64(51): np.int64(93),\n",
      "                            np.int64(52): np.int64(23),\n",
      "                            np.int64(53): np.int64(140),\n",
      "                            np.int64(54): np.int64(50),\n",
      "                            np.int64(55): np.int64(55),\n",
      "                            np.int64(56): np.int64(3),\n",
      "                            np.int64(57): np.int64(177),\n",
      "                            np.int64(58): np.int64(3),\n",
      "                            np.int64(60): np.int64(39),\n",
      "                            np.int64(61): np.int64(11),\n",
      "                            np.int64(62): np.int64(9),\n",
      "                            np.int64(63): np.int64(94),\n",
      "                            np.int64(64): np.int64(18),\n",
      "                            np.int64(65): np.int64(28),\n",
      "                            np.int64(66): np.int64(151),\n",
      "                            np.int64(67): np.int64(68),\n",
      "                            np.int64(68): np.int64(2),\n",
      "                            np.int64(69): np.int64(9),\n",
      "                            np.int64(70): np.int64(8),\n",
      "                            np.int64(71): np.int64(74),\n",
      "                            np.int64(72): np.int64(199),\n",
      "                            np.int64(73): np.int64(9),\n",
      "                            np.int64(74): np.int64(6),\n",
      "                            np.int64(75): np.int64(37),\n",
      "                            np.int64(76): np.int64(9),\n",
      "                            np.int64(77): np.int64(168),\n",
      "                            np.int64(78): np.int64(136),\n",
      "                            np.int64(79): np.int64(11),\n",
      "                            np.int64(80): np.int64(48),\n",
      "                            np.int64(81): np.int64(29),\n",
      "                            np.int64(82): np.int64(2),\n",
      "                            np.int64(83): np.int64(7),\n",
      "                            np.int64(84): np.int64(59),\n",
      "                            np.int64(85): np.int64(27),\n",
      "                            np.int64(86): np.int64(34),\n",
      "                            np.int64(87): np.int64(6),\n",
      "                            np.int64(88): np.int64(2),\n",
      "                            np.int64(89): np.int64(4),\n",
      "                            np.int64(90): np.int64(19),\n",
      "                            np.int64(91): np.int64(3),\n",
      "                            np.int64(92): np.int64(5),\n",
      "                            np.int64(93): np.int64(9),\n",
      "                            np.int64(94): np.int64(147),\n",
      "                            np.int64(95): np.int64(308),\n",
      "                            np.int64(96): np.int64(15),\n",
      "                            np.int64(97): np.int64(80),\n",
      "                            np.int64(98): np.int64(61)},\n",
      "     'gradient_norm': 5.7277021408081055,\n",
      "     'learning_rate': 0.01,\n",
      "     'local_epochs': 1,\n",
      "     'local_loss': 3.057929515838623,\n",
      "     'num_samples': 5622,\n",
      "     'per_layer_norms': {'bn1.bias': 0.07169351726770401,\n",
      "                         'bn1.weight': 0.11588869243860245,\n",
      "                         'conv1.weight': 1.2626618146896362,\n",
      "                         'fc.bias': 0.17713765799999237,\n",
      "                         'fc.weight': 3.003577709197998,\n",
      "                         'layer1.0.bn1.bias': 0.06437837332487106,\n",
      "                         'layer1.0.bn1.weight': 0.07735550403594971,\n",
      "                         'layer1.0.bn2.bias': 0.04040510207414627,\n",
      "                         'layer1.0.bn2.weight': 0.07589872926473618,\n",
      "                         'layer1.0.conv1.weight': 0.9714843034744263,\n",
      "                         'layer1.0.conv2.weight': 0.9418813586235046,\n",
      "                         'layer1.1.bn1.bias': 0.04224254935979843,\n",
      "                         'layer1.1.bn1.weight': 0.043823495507240295,\n",
      "                         'layer1.1.bn2.bias': 0.028590001165866852,\n",
      "                         'layer1.1.bn2.weight': 0.05636874586343765,\n",
      "                         'layer1.1.conv1.weight': 0.8100247383117676,\n",
      "                         'layer1.1.conv2.weight': 0.809881865978241,\n",
      "                         'layer2.0.bn1.bias': 0.05140746012330055,\n",
      "                         'layer2.0.bn1.weight': 0.060257688164711,\n",
      "                         'layer2.0.bn2.bias': 0.047314029186964035,\n",
      "                         'layer2.0.bn2.weight': 0.057230040431022644,\n",
      "                         'layer2.0.conv1.weight': 1.126690149307251,\n",
      "                         'layer2.0.conv2.weight': 1.2015880346298218,\n",
      "                         'layer2.0.downsample.0.weight': 0.47396740317344666,\n",
      "                         'layer2.0.downsample.1.bias': 0.047314029186964035,\n",
      "                         'layer2.0.downsample.1.weight': 0.05753149464726448,\n",
      "                         'layer2.1.bn1.bias': 0.04428165405988693,\n",
      "                         'layer2.1.bn1.weight': 0.048363249748945236,\n",
      "                         'layer2.1.bn2.bias': 0.030941873788833618,\n",
      "                         'layer2.1.bn2.weight': 0.04258996620774269,\n",
      "                         'layer2.1.conv1.weight': 1.1555299758911133,\n",
      "                         'layer2.1.conv2.weight': 1.063283920288086,\n",
      "                         'layer3.0.bn1.bias': 0.04060721769928932,\n",
      "                         'layer3.0.bn1.weight': 0.04997404292225838,\n",
      "                         'layer3.0.bn2.bias': 0.036700647324323654,\n",
      "                         'layer3.0.bn2.weight': 0.05549614876508713,\n",
      "                         'layer3.0.conv1.weight': 1.4728009700775146,\n",
      "                         'layer3.0.conv2.weight': 1.3799173831939697,\n",
      "                         'layer3.0.downsample.0.weight': 0.5161852240562439,\n",
      "                         'layer3.0.downsample.1.bias': 0.036700647324323654,\n",
      "                         'layer3.0.downsample.1.weight': 0.044986940920352936,\n",
      "                         'layer3.1.bn1.bias': 0.03079185262322426,\n",
      "                         'layer3.1.bn1.weight': 0.029841620475053787,\n",
      "                         'layer3.1.bn2.bias': 0.025644170120358467,\n",
      "                         'layer3.1.bn2.weight': 0.040008652955293655,\n",
      "                         'layer3.1.conv1.weight': 1.1400178670883179,\n",
      "                         'layer3.1.conv2.weight': 1.0146666765213013,\n",
      "                         'layer4.0.bn1.bias': 0.02991800382733345,\n",
      "                         'layer4.0.bn1.weight': 0.03484374284744263,\n",
      "                         'layer4.0.bn2.bias': 0.06642162799835205,\n",
      "                         'layer4.0.bn2.weight': 0.060954395681619644,\n",
      "                         'layer4.0.conv1.weight': 1.3200520277023315,\n",
      "                         'layer4.0.conv2.weight': 1.3457008600234985,\n",
      "                         'layer4.0.downsample.0.weight': 0.8080634474754333,\n",
      "                         'layer4.0.downsample.1.bias': 0.06642162799835205,\n",
      "                         'layer4.0.downsample.1.weight': 0.06681131571531296,\n",
      "                         'layer4.1.bn1.bias': 0.025310901924967766,\n",
      "                         'layer4.1.bn1.weight': 0.02841605246067047,\n",
      "                         'layer4.1.bn2.bias': 0.08446820825338364,\n",
      "                         'layer4.1.bn2.weight': 0.05139230936765671,\n",
      "                         'layer4.1.conv1.weight': 1.130314826965332,\n",
      "                         'layer4.1.conv2.weight': 1.1598137617111206}},\n",
      " 7: {'batch_size': 64,\n",
      "     'class_distribution': {np.int64(0): np.int64(2),\n",
      "                            np.int64(1): np.int64(44),\n",
      "                            np.int64(2): np.int64(104),\n",
      "                            np.int64(3): np.int64(3),\n",
      "                            np.int64(4): np.int64(2),\n",
      "                            np.int64(5): np.int64(48),\n",
      "                            np.int64(6): np.int64(17),\n",
      "                            np.int64(7): np.int64(2),\n",
      "                            np.int64(8): np.int64(79),\n",
      "                            np.int64(9): np.int64(9),\n",
      "                            np.int64(10): np.int64(103),\n",
      "                            np.int64(11): np.int64(30),\n",
      "                            np.int64(12): np.int64(106),\n",
      "                            np.int64(13): np.int64(9),\n",
      "                            np.int64(14): np.int64(8),\n",
      "                            np.int64(15): np.int64(39),\n",
      "                            np.int64(16): np.int64(5),\n",
      "                            np.int64(17): np.int64(46),\n",
      "                            np.int64(18): np.int64(89),\n",
      "                            np.int64(20): np.int64(1),\n",
      "                            np.int64(21): np.int64(72),\n",
      "                            np.int64(22): np.int64(22),\n",
      "                            np.int64(23): np.int64(59),\n",
      "                            np.int64(24): np.int64(17),\n",
      "                            np.int64(25): np.int64(38),\n",
      "                            np.int64(26): np.int64(111),\n",
      "                            np.int64(27): np.int64(1),\n",
      "                            np.int64(28): np.int64(9),\n",
      "                            np.int64(29): np.int64(5),\n",
      "                            np.int64(30): np.int64(6),\n",
      "                            np.int64(31): np.int64(133),\n",
      "                            np.int64(32): np.int64(14),\n",
      "                            np.int64(34): np.int64(26),\n",
      "                            np.int64(35): np.int64(34),\n",
      "                            np.int64(36): np.int64(10),\n",
      "                            np.int64(37): np.int64(145),\n",
      "                            np.int64(38): np.int64(31),\n",
      "                            np.int64(39): np.int64(1),\n",
      "                            np.int64(40): np.int64(46),\n",
      "                            np.int64(41): np.int64(63),\n",
      "                            np.int64(42): np.int64(44),\n",
      "                            np.int64(43): np.int64(110),\n",
      "                            np.int64(44): np.int64(109),\n",
      "                            np.int64(45): np.int64(47),\n",
      "                            np.int64(46): np.int64(102),\n",
      "                            np.int64(47): np.int64(33),\n",
      "                            np.int64(48): np.int64(28),\n",
      "                            np.int64(49): np.int64(52),\n",
      "                            np.int64(50): np.int64(159),\n",
      "                            np.int64(51): np.int64(1),\n",
      "                            np.int64(52): np.int64(56),\n",
      "                            np.int64(53): np.int64(55),\n",
      "                            np.int64(54): np.int64(163),\n",
      "                            np.int64(55): np.int64(1),\n",
      "                            np.int64(56): np.int64(90),\n",
      "                            np.int64(57): np.int64(41),\n",
      "                            np.int64(59): np.int64(4),\n",
      "                            np.int64(62): np.int64(4),\n",
      "                            np.int64(63): np.int64(78),\n",
      "                            np.int64(64): np.int64(134),\n",
      "                            np.int64(65): np.int64(79),\n",
      "                            np.int64(67): np.int64(7),\n",
      "                            np.int64(68): np.int64(282),\n",
      "                            np.int64(69): np.int64(3),\n",
      "                            np.int64(70): np.int64(146),\n",
      "                            np.int64(72): np.int64(119),\n",
      "                            np.int64(73): np.int64(40),\n",
      "                            np.int64(74): np.int64(78),\n",
      "                            np.int64(75): np.int64(87),\n",
      "                            np.int64(76): np.int64(12),\n",
      "                            np.int64(77): np.int64(76),\n",
      "                            np.int64(78): np.int64(10),\n",
      "                            np.int64(80): np.int64(40),\n",
      "                            np.int64(81): np.int64(7),\n",
      "                            np.int64(82): np.int64(216),\n",
      "                            np.int64(83): np.int64(13),\n",
      "                            np.int64(84): np.int64(4),\n",
      "                            np.int64(85): np.int64(11),\n",
      "                            np.int64(86): np.int64(2),\n",
      "                            np.int64(87): np.int64(2),\n",
      "                            np.int64(88): np.int64(14),\n",
      "                            np.int64(89): np.int64(81),\n",
      "                            np.int64(90): np.int64(44),\n",
      "                            np.int64(91): np.int64(61),\n",
      "                            np.int64(92): np.int64(111),\n",
      "                            np.int64(93): np.int64(20),\n",
      "                            np.int64(94): np.int64(59),\n",
      "                            np.int64(96): np.int64(8),\n",
      "                            np.int64(97): np.int64(36),\n",
      "                            np.int64(98): np.int64(131),\n",
      "                            np.int64(99): np.int64(54)},\n",
      "     'gradient_norm': 5.601260185241699,\n",
      "     'learning_rate': 0.01,\n",
      "     'local_epochs': 1,\n",
      "     'local_loss': 2.473963499069214,\n",
      "     'num_samples': 4753,\n",
      "     'per_layer_norms': {'bn1.bias': 0.06478703022003174,\n",
      "                         'bn1.weight': 0.15708310902118683,\n",
      "                         'conv1.weight': 1.2734787464141846,\n",
      "                         'fc.bias': 0.15835130214691162,\n",
      "                         'fc.weight': 2.904043197631836,\n",
      "                         'layer1.0.bn1.bias': 0.06672374904155731,\n",
      "                         'layer1.0.bn1.weight': 0.07532451301813126,\n",
      "                         'layer1.0.bn2.bias': 0.05197032913565636,\n",
      "                         'layer1.0.bn2.weight': 0.08778281509876251,\n",
      "                         'layer1.0.conv1.weight': 1.0159910917282104,\n",
      "                         'layer1.0.conv2.weight': 0.9504621624946594,\n",
      "                         'layer1.1.bn1.bias': 0.045871224254369736,\n",
      "                         'layer1.1.bn1.weight': 0.051882628351449966,\n",
      "                         'layer1.1.bn2.bias': 0.030070781707763672,\n",
      "                         'layer1.1.bn2.weight': 0.05314082279801369,\n",
      "                         'layer1.1.conv1.weight': 0.8012272715568542,\n",
      "                         'layer1.1.conv2.weight': 0.7977421283721924,\n",
      "                         'layer2.0.bn1.bias': 0.05082852765917778,\n",
      "                         'layer2.0.bn1.weight': 0.06165444478392601,\n",
      "                         'layer2.0.bn2.bias': 0.04355061426758766,\n",
      "                         'layer2.0.bn2.weight': 0.06333856284618378,\n",
      "                         'layer2.0.conv1.weight': 1.1159769296646118,\n",
      "                         'layer2.0.conv2.weight': 1.2020225524902344,\n",
      "                         'layer2.0.downsample.0.weight': 0.46437081694602966,\n",
      "                         'layer2.0.downsample.1.bias': 0.04355061426758766,\n",
      "                         'layer2.0.downsample.1.weight': 0.05538886785507202,\n",
      "                         'layer2.1.bn1.bias': 0.04061040282249451,\n",
      "                         'layer2.1.bn1.weight': 0.050105541944503784,\n",
      "                         'layer2.1.bn2.bias': 0.03310022130608559,\n",
      "                         'layer2.1.bn2.weight': 0.05258002504706383,\n",
      "                         'layer2.1.conv1.weight': 1.1390494108200073,\n",
      "                         'layer2.1.conv2.weight': 1.0279752016067505,\n",
      "                         'layer3.0.bn1.bias': 0.03880870342254639,\n",
      "                         'layer3.0.bn1.weight': 0.04952004924416542,\n",
      "                         'layer3.0.bn2.bias': 0.03436407074332237,\n",
      "                         'layer3.0.bn2.weight': 0.05237727239727974,\n",
      "                         'layer3.0.conv1.weight': 1.4354058504104614,\n",
      "                         'layer3.0.conv2.weight': 1.3180292844772339,\n",
      "                         'layer3.0.downsample.0.weight': 0.5071263313293457,\n",
      "                         'layer3.0.downsample.1.bias': 0.03436407074332237,\n",
      "                         'layer3.0.downsample.1.weight': 0.04539880156517029,\n",
      "                         'layer3.1.bn1.bias': 0.028745215386152267,\n",
      "                         'layer3.1.bn1.weight': 0.03132724389433861,\n",
      "                         'layer3.1.bn2.bias': 0.0219151321798563,\n",
      "                         'layer3.1.bn2.weight': 0.03630504384636879,\n",
      "                         'layer3.1.conv1.weight': 1.0938591957092285,\n",
      "                         'layer3.1.conv2.weight': 0.9590808153152466,\n",
      "                         'layer4.0.bn1.bias': 0.03060857020318508,\n",
      "                         'layer4.0.bn1.weight': 0.03722846508026123,\n",
      "                         'layer4.0.bn2.bias': 0.06943848729133606,\n",
      "                         'layer4.0.bn2.weight': 0.06298378109931946,\n",
      "                         'layer4.0.conv1.weight': 1.274207353591919,\n",
      "                         'layer4.0.conv2.weight': 1.2997519969940186,\n",
      "                         'layer4.0.downsample.0.weight': 0.7764663696289062,\n",
      "                         'layer4.0.downsample.1.bias': 0.06943848729133606,\n",
      "                         'layer4.0.downsample.1.weight': 0.06547775119543076,\n",
      "                         'layer4.1.bn1.bias': 0.028518734499812126,\n",
      "                         'layer4.1.bn1.weight': 0.032681163400411606,\n",
      "                         'layer4.1.bn2.bias': 0.08996334671974182,\n",
      "                         'layer4.1.bn2.weight': 0.057756077498197556,\n",
      "                         'layer4.1.conv1.weight': 1.1104384660720825,\n",
      "                         'layer4.1.conv2.weight': 1.1697193384170532}},\n",
      " 9: {'batch_size': 64,\n",
      "     'class_distribution': {np.int64(0): np.int64(36),\n",
      "                            np.int64(1): np.int64(14),\n",
      "                            np.int64(2): np.int64(3),\n",
      "                            np.int64(3): np.int64(39),\n",
      "                            np.int64(4): np.int64(1),\n",
      "                            np.int64(5): np.int64(2),\n",
      "                            np.int64(6): np.int64(36),\n",
      "                            np.int64(7): np.int64(1),\n",
      "                            np.int64(8): np.int64(250),\n",
      "                            np.int64(9): np.int64(120),\n",
      "                            np.int64(10): np.int64(14),\n",
      "                            np.int64(11): np.int64(78),\n",
      "                            np.int64(12): np.int64(7),\n",
      "                            np.int64(13): np.int64(164),\n",
      "                            np.int64(14): np.int64(14),\n",
      "                            np.int64(15): np.int64(30),\n",
      "                            np.int64(16): np.int64(104),\n",
      "                            np.int64(17): np.int64(3),\n",
      "                            np.int64(18): np.int64(29),\n",
      "                            np.int64(19): np.int64(81),\n",
      "                            np.int64(20): np.int64(4),\n",
      "                            np.int64(21): np.int64(1),\n",
      "                            np.int64(22): np.int64(112),\n",
      "                            np.int64(23): np.int64(9),\n",
      "                            np.int64(24): np.int64(14),\n",
      "                            np.int64(25): np.int64(37),\n",
      "                            np.int64(26): np.int64(16),\n",
      "                            np.int64(27): np.int64(105),\n",
      "                            np.int64(28): np.int64(24),\n",
      "                            np.int64(29): np.int64(1),\n",
      "                            np.int64(30): np.int64(35),\n",
      "                            np.int64(31): np.int64(3),\n",
      "                            np.int64(32): np.int64(11),\n",
      "                            np.int64(33): np.int64(30),\n",
      "                            np.int64(34): np.int64(6),\n",
      "                            np.int64(35): np.int64(51),\n",
      "                            np.int64(36): np.int64(25),\n",
      "                            np.int64(37): np.int64(1),\n",
      "                            np.int64(38): np.int64(36),\n",
      "                            np.int64(39): np.int64(5),\n",
      "                            np.int64(40): np.int64(3),\n",
      "                            np.int64(41): np.int64(35),\n",
      "                            np.int64(42): np.int64(6),\n",
      "                            np.int64(43): np.int64(15),\n",
      "                            np.int64(44): np.int64(73),\n",
      "                            np.int64(45): np.int64(87),\n",
      "                            np.int64(46): np.int64(44),\n",
      "                            np.int64(47): np.int64(45),\n",
      "                            np.int64(48): np.int64(5),\n",
      "                            np.int64(49): np.int64(97),\n",
      "                            np.int64(50): np.int64(23),\n",
      "                            np.int64(51): np.int64(226),\n",
      "                            np.int64(52): np.int64(24),\n",
      "                            np.int64(53): np.int64(107),\n",
      "                            np.int64(54): np.int64(7),\n",
      "                            np.int64(55): np.int64(137),\n",
      "                            np.int64(56): np.int64(108),\n",
      "                            np.int64(57): np.int64(7),\n",
      "                            np.int64(58): np.int64(4),\n",
      "                            np.int64(59): np.int64(11),\n",
      "                            np.int64(60): np.int64(1),\n",
      "                            np.int64(61): np.int64(24),\n",
      "                            np.int64(62): np.int64(1),\n",
      "                            np.int64(63): np.int64(5),\n",
      "                            np.int64(64): np.int64(2),\n",
      "                            np.int64(65): np.int64(13),\n",
      "                            np.int64(66): np.int64(98),\n",
      "                            np.int64(67): np.int64(100),\n",
      "                            np.int64(68): np.int64(1),\n",
      "                            np.int64(69): np.int64(284),\n",
      "                            np.int64(70): np.int64(25),\n",
      "                            np.int64(71): np.int64(20),\n",
      "                            np.int64(72): np.int64(72),\n",
      "                            np.int64(73): np.int64(11),\n",
      "                            np.int64(74): np.int64(27),\n",
      "                            np.int64(75): np.int64(6),\n",
      "                            np.int64(76): np.int64(130),\n",
      "                            np.int64(77): np.int64(10),\n",
      "                            np.int64(78): np.int64(152),\n",
      "                            np.int64(79): np.int64(28),\n",
      "                            np.int64(80): np.int64(37),\n",
      "                            np.int64(81): np.int64(50),\n",
      "                            np.int64(82): np.int64(12),\n",
      "                            np.int64(83): np.int64(23),\n",
      "                            np.int64(84): np.int64(23),\n",
      "                            np.int64(85): np.int64(5),\n",
      "                            np.int64(86): np.int64(102),\n",
      "                            np.int64(87): np.int64(17),\n",
      "                            np.int64(88): np.int64(229),\n",
      "                            np.int64(89): np.int64(2),\n",
      "                            np.int64(90): np.int64(74),\n",
      "                            np.int64(91): np.int64(9),\n",
      "                            np.int64(92): np.int64(28),\n",
      "                            np.int64(93): np.int64(2),\n",
      "                            np.int64(94): np.int64(8),\n",
      "                            np.int64(95): np.int64(3),\n",
      "                            np.int64(96): np.int64(265),\n",
      "                            np.int64(97): np.int64(7),\n",
      "                            np.int64(98): np.int64(125),\n",
      "                            np.int64(99): np.int64(319)},\n",
      "     'gradient_norm': 5.950563907623291,\n",
      "     'learning_rate': 0.01,\n",
      "     'local_epochs': 1,\n",
      "     'local_loss': 2.876342296600342,\n",
      "     'num_samples': 4966,\n",
      "     'per_layer_norms': {'bn1.bias': 0.07096107304096222,\n",
      "                         'bn1.weight': 0.14023591578006744,\n",
      "                         'conv1.weight': 1.7246252298355103,\n",
      "                         'fc.bias': 0.15798291563987732,\n",
      "                         'fc.weight': 3.0077176094055176,\n",
      "                         'layer1.0.bn1.bias': 0.05843876674771309,\n",
      "                         'layer1.0.bn1.weight': 0.057489387691020966,\n",
      "                         'layer1.0.bn2.bias': 0.0503271147608757,\n",
      "                         'layer1.0.bn2.weight': 0.07167072594165802,\n",
      "                         'layer1.0.conv1.weight': 0.9762300252914429,\n",
      "                         'layer1.0.conv2.weight': 0.9193488359451294,\n",
      "                         'layer1.1.bn1.bias': 0.04559565335512161,\n",
      "                         'layer1.1.bn1.weight': 0.056514922529459,\n",
      "                         'layer1.1.bn2.bias': 0.032743073999881744,\n",
      "                         'layer1.1.bn2.weight': 0.057570118457078934,\n",
      "                         'layer1.1.conv1.weight': 0.8313138484954834,\n",
      "                         'layer1.1.conv2.weight': 0.8095924258232117,\n",
      "                         'layer2.0.bn1.bias': 0.04738692194223404,\n",
      "                         'layer2.0.bn1.weight': 0.05363352224230766,\n",
      "                         'layer2.0.bn2.bias': 0.050031423568725586,\n",
      "                         'layer2.0.bn2.weight': 0.059431660920381546,\n",
      "                         'layer2.0.conv1.weight': 1.1345518827438354,\n",
      "                         'layer2.0.conv2.weight': 1.217913269996643,\n",
      "                         'layer2.0.downsample.0.weight': 0.47433236241340637,\n",
      "                         'layer2.0.downsample.1.bias': 0.050031423568725586,\n",
      "                         'layer2.0.downsample.1.weight': 0.06580259650945663,\n",
      "                         'layer2.1.bn1.bias': 0.04833603277802467,\n",
      "                         'layer2.1.bn1.weight': 0.05547209456562996,\n",
      "                         'layer2.1.bn2.bias': 0.03292734548449516,\n",
      "                         'layer2.1.bn2.weight': 0.05077572911977768,\n",
      "                         'layer2.1.conv1.weight': 1.1915459632873535,\n",
      "                         'layer2.1.conv2.weight': 1.0977251529693604,\n",
      "                         'layer3.0.bn1.bias': 0.04258083552122116,\n",
      "                         'layer3.0.bn1.weight': 0.05393457040190697,\n",
      "                         'layer3.0.bn2.bias': 0.04047265648841858,\n",
      "                         'layer3.0.bn2.weight': 0.05382877588272095,\n",
      "                         'layer3.0.conv1.weight': 1.501116394996643,\n",
      "                         'layer3.0.conv2.weight': 1.4036847352981567,\n",
      "                         'layer3.0.downsample.0.weight': 0.5415886640548706,\n",
      "                         'layer3.0.downsample.1.bias': 0.04047265648841858,\n",
      "                         'layer3.0.downsample.1.weight': 0.04901336506009102,\n",
      "                         'layer3.1.bn1.bias': 0.029236378148198128,\n",
      "                         'layer3.1.bn1.weight': 0.0346459224820137,\n",
      "                         'layer3.1.bn2.bias': 0.023772068321704865,\n",
      "                         'layer3.1.bn2.weight': 0.03715433180332184,\n",
      "                         'layer3.1.conv1.weight': 1.1666513681411743,\n",
      "                         'layer3.1.conv2.weight': 1.0312989950180054,\n",
      "                         'layer4.0.bn1.bias': 0.02929883822798729,\n",
      "                         'layer4.0.bn1.weight': 0.03500824794173241,\n",
      "                         'layer4.0.bn2.bias': 0.06993598490953445,\n",
      "                         'layer4.0.bn2.weight': 0.06575430929660797,\n",
      "                         'layer4.0.conv1.weight': 1.3861925601959229,\n",
      "                         'layer4.0.conv2.weight': 1.3979415893554688,\n",
      "                         'layer4.0.downsample.0.weight': 0.7950114011764526,\n",
      "                         'layer4.0.downsample.1.bias': 0.06993598490953445,\n",
      "                         'layer4.0.downsample.1.weight': 0.07093555480241776,\n",
      "                         'layer4.1.bn1.bias': 0.030522344633936882,\n",
      "                         'layer4.1.bn1.weight': 0.032201655209064484,\n",
      "                         'layer4.1.bn2.bias': 0.09266397356987,\n",
      "                         'layer4.1.bn2.weight': 0.0582992322742939,\n",
      "                         'layer4.1.conv1.weight': 1.2023974657058716,\n",
      "                         'layer4.1.conv2.weight': 1.2549879550933838}}}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(metrics_to_export[\"client_metrics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c72f77",
   "metadata": {},
   "source": [
    "Saving the full object locally (so we can load it later for analysis or privacy-attack experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbb5874",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"fed_metrics_round20.pkl\", \"wb\") as f:\n",
    "    pickle.dump(metrics_to_export, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e704473",
   "metadata": {},
   "source": [
    "Then we can reload later with:\n",
    "\n",
    "\"\n",
    "\n",
    "with open(\"fed_metrics_round20.pkl\", \"rb\") as f:\n",
    "    m = pickle.load(f)\n",
    "\n",
    "\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl_privacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
