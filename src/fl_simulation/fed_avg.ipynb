{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "426f71b6-ed5c-47f9-9a35-f84920d83a5e",
   "metadata": {},
   "source": [
    "# Federated Averaging (FedAvg) baseline on CIFAR-100\n",
    "- Dirichlet non-IID partitioning\n",
    "- Partial client participation\n",
    "- Optional heterogeneity (per-client batch size / epochs / lr)\n",
    "- Safe, self-contained, single-file version\n",
    "## Imports and simulation defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fb67608-4ff3-48de-a18c-c521e3ed9f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "# Reproducibility\n",
    "rng_seed = 42\n",
    "random.seed(rng_seed)\n",
    "np.random.seed(rng_seed)\n",
    "torch.manual_seed(rng_seed)  \n",
    "# (we can also seed CUDA later if present: torch.cuda.manual_seed_all(rng_seed))\n",
    "\n",
    "# Device helper (prefers CUDA, then MPS, else CPU)\n",
    "def get_device(prefer: Optional[str] = None) -> torch.device:\n",
    "    if prefer == \"cuda\" and torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if prefer == \"mps\" and getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640693cd-8558-4039-94a6-73e1c7310bb8",
   "metadata": {},
   "source": [
    "### Data: CIFAR-100 loaders (train/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b4d32ec-704c-495b-8df7-aae896d1af55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar100(data_root: str = \"./data\"):\n",
    "    # Mild augmentation on train; standard normalization\n",
    "    mean = (0.5071, 0.4867, 0.4408)\n",
    "    std = (0.2675, 0.2565, 0.2761)\n",
    "\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "    test_tf = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "\n",
    "    train = datasets.CIFAR100(root=data_root, train=True, download=True, transform=train_tf)\n",
    "    test = datasets.CIFAR100(root=data_root, train=False, download=True, transform=test_tf)\n",
    "    return train, test  # mirrors your original intent. \n",
    "\n",
    "\n",
    "# %%\n",
    "# Targets accessor (handles .targets/.labels)\n",
    "def _get_targets(dataset) -> np.ndarray:\n",
    "    targets = getattr(dataset, \"targets\", None)\n",
    "    if targets is None:\n",
    "        targets = getattr(dataset, \"labels\", None)\n",
    "    if targets is None:\n",
    "        raise AttributeError(\"Dataset has no 'targets' or 'labels'.\")\n",
    "    return np.array(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f347b8f-a81e-4fbb-a9a2-5618ed7d6fbd",
   "metadata": {},
   "source": [
    "## Dirichlet non-IID split (returns dict: client_id -> list of indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e02627af-60ba-44f7-bf5f-00cd33ebb247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dirichlet_noniid_indices(dataset, num_clients: int, alpha: float, min_per_client: int = 10) -> Dict[int, List[int]]:\n",
    "    y = _get_targets(dataset)\n",
    "    num_classes = int(y.max()) + 1\n",
    "    idx_by_class = {c: np.where(y == c)[0] for c in range(num_classes)}\n",
    "    for c in idx_by_class:\n",
    "        np.random.shuffle(idx_by_class[c])\n",
    "\n",
    "    # Each class's indices are split among clients with Dirichlet(alpha)\n",
    "    client_indices = [[] for _ in range(num_clients)]\n",
    "    for c in range(num_classes):\n",
    "        idx_c = idx_by_class[c]\n",
    "        if len(idx_c) == 0:\n",
    "            continue\n",
    "        # Dirichlet draw for this class across clients\n",
    "        p = np.random.dirichlet([alpha] * num_clients)\n",
    "        # Proportions -> integer split (rounding by cumulative sums)\n",
    "        cuts = (np.cumsum(p) * len(idx_c)).astype(int)[:-1]\n",
    "        split = np.split(idx_c, cuts)\n",
    "        for i, shard in enumerate(split):\n",
    "            client_indices[i].extend(shard.tolist())\n",
    "\n",
    "    # Ensure minimum per client (fallback to random fill if some are tiny)\n",
    "    # This usually isn't necessary for reasonable alpha, but keeps loaders happy\n",
    "    pool = list(range(len(dataset)))\n",
    "    for i in range(num_clients):\n",
    "        if len(client_indices[i]) < min_per_client:\n",
    "            need = min_per_client - len(client_indices[i])\n",
    "            extra = np.random.choice(pool, size=need, replace=False).tolist()\n",
    "            client_indices[i].extend(extra)\n",
    "\n",
    "    # Shuffle each client's order (nicer batching)\n",
    "    for i in range(num_clients):\n",
    "        random.shuffle(client_indices[i])\n",
    "    return {i: client_indices[i] for i in range(num_clients)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1ab77a-7051-4c8a-a378-17572b213790",
   "metadata": {},
   "source": [
    "## Model: ResNet18 head for CIFAR-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82b48ab7-a3ea-4203-b42d-de354332bd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes: int = 100) -> nn.Module:\n",
    "    model = models.resnet18(weights=None)  # no pretrained to avoid download in restricted envs\n",
    "    # CIFAR images are 3x32x32; torchvision ResNet expects 224x224,\n",
    "    # but it's fineâ€”ResNet is fully conv except FC. It still works on 32x32.\n",
    "    # Replace final FC layer to match number of classes\n",
    "    in_feats = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_feats, num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "# %%\n",
    "def evaluate(model: nn.Module, loader: DataLoader, device: torch.device) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_sum = 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss_sum += loss.item() * x.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += x.size(0)\n",
    "    avg_loss = loss_sum / max(1, total)\n",
    "    acc = correct / max(1, total)\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "# %%\n",
    "def local_train(\n",
    "    global_model: nn.Module,\n",
    "    subset: Subset,\n",
    "    device: torch.device,\n",
    "    epochs: int = 1,\n",
    "    batch_size: int = 64,\n",
    "    lr: float = 0.01,\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    model = copy.deepcopy(global_model).to(device)\n",
    "    loader = DataLoader(subset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=False)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "    model.train()\n",
    "    for _ in range(max(1, epochs)):\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Return CPU weights to simulate uplink\n",
    "    return {k: v.detach().cpu() for k, v in model.state_dict().items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b2afc1-e8b4-4d9f-ba4f-0ade607afdfb",
   "metadata": {},
   "source": [
    "## Weighted model averaging (FedAvg) with checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2643eeb-173f-4794-a397-040687ca1ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_weights(weight_list, sizes):\n",
    "    if not weight_list:\n",
    "        raise ValueError(\"No client weights provided.\")\n",
    "    if len(weight_list) != len(sizes):\n",
    "        raise ValueError(\"weights and sizes mismatch\")\n",
    "    total = float(sum(sizes))\n",
    "    avg = {k: torch.zeros_like(v) for k, v in weight_list[0].items()}\n",
    "\n",
    "    for wi, si in zip(weight_list, sizes):\n",
    "        w = si / total\n",
    "        for k in avg.keys():\n",
    "            if avg[k].dtype.is_floating_point:      \n",
    "                avg[k] += wi[k].float() * w\n",
    "            else:\n",
    "                avg[k] = wi[k].clone()              \n",
    "\n",
    "    return avg # mirrors safety we intend. :contentReference[oaicite:2]{index=2}\n",
    "\n",
    "# %%\n",
    "def federated_training(\n",
    "    train_dataset,\n",
    "    test_dataset,\n",
    "    partitions: Dict[int, List[int]],\n",
    "    rounds: int = 10,\n",
    "    local_epochs: int = 1,\n",
    "    device: str = \"cpu\",\n",
    "    q: float = 1.0,  # participation rate per round\n",
    "    num_classes: int = 100,\n",
    "    batch_size: int = 64,\n",
    "    lr: float = 0.01,\n",
    "    hetero_profiles: Optional[Dict[int, Dict[str, float]]] = None,  # per-client overrides\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the global model using FedAvg over 'rounds' communication rounds.\n",
    "    \"\"\"\n",
    "    # Validate partitions\n",
    "    num_clients = len(partitions)\n",
    "    if num_clients == 0:\n",
    "        raise ValueError(\"No clients available for federated training.\")\n",
    "    device = get_device(device)\n",
    "\n",
    "    # Build global model\n",
    "    global_model = build_model(num_classes=num_classes).to(device)\n",
    "\n",
    "    # Test loader (global test set)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=2)\n",
    "\n",
    "    history = {\"round\": [], \"test_loss\": [], \"test_acc\": [], \"selected\": []}\n",
    "\n",
    "    for r in range(1, rounds + 1):\n",
    "        # Client sampling\n",
    "        m = max(1, int(math.ceil(q * num_clients)))\n",
    "        selected = sorted(random.sample(range(num_clients), m))\n",
    "\n",
    "        # Local updates\n",
    "        local_weights = []\n",
    "        local_sizes = []\n",
    "        for cid in selected:\n",
    "            idxs = partitions[cid]\n",
    "            subset = Subset(train_dataset, idxs)\n",
    "\n",
    "            # Per-client heterogeneity overrides if provided\n",
    "            ep = local_epochs\n",
    "            bs = batch_size\n",
    "            lr_i = lr\n",
    "            if hetero_profiles and cid in hetero_profiles:\n",
    "                ep = int(hetero_profiles[cid].get(\"epochs\", ep))\n",
    "                bs = int(hetero_profiles[cid].get(\"batch_size\", bs))\n",
    "                lr_i = float(hetero_profiles[cid].get(\"lr\", lr_i))\n",
    "\n",
    "            wi = local_train(global_model, subset, device, epochs=ep, batch_size=bs, lr=lr_i)\n",
    "            local_weights.append(wi)\n",
    "            local_sizes.append(len(subset))\n",
    "\n",
    "        # FedAvg\n",
    "        new_state = average_weights(local_weights, local_sizes)\n",
    "        global_model.load_state_dict(new_state)\n",
    "\n",
    "        # Evaluate global model\n",
    "        test_loss, test_acc = evaluate(global_model, test_loader, device)\n",
    "        history[\"round\"].append(r)\n",
    "        history[\"test_loss\"].append(test_loss)\n",
    "        history[\"test_acc\"].append(test_acc)\n",
    "        history[\"selected\"].append(selected)\n",
    "\n",
    "        print(f\"[Round {r:03d}] Test loss: {test_loss:.4f} | Test acc: {test_acc*100:.2f}% | Clients: {selected}\")\n",
    "\n",
    "    return global_model, history  # same signature goal you had. :contentReference[oaicite:3]{index=3}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb72276a-9d1b-47fe-8d8d-014832f70ea4",
   "metadata": {},
   "source": [
    "## Final "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e155dac-a8ee-45c5-bd73-f1a63236d331",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # --- Config (kept close to your defaults) ---\n",
    "    number_of_clients = 10\n",
    "    number_of_rounds = 20\n",
    "    local_epochs = 1            # start small for smoke tests; raise later\n",
    "    participation = 0.75        # fraction of clients per round\n",
    "    dirichlet_alpha = 0.5       # non-IID strength (lower => more skew)\n",
    "    batch_size = 64\n",
    "    lr = 0.01\n",
    "    device_pref = None          # \"cuda\" | \"mps\" | \"cpu\" | None (auto)\n",
    "\n",
    "    # (These reflect the same values/types you were using.) :contentReference[oaicite:4]{index=4}\n",
    "\n",
    "    # --- Data ---\n",
    "    train_dataset, test_dataset = load_cifar100()\n",
    "\n",
    "    # --- Partitioning ---\n",
    "    partitions = dirichlet_noniid_indices(\n",
    "        train_dataset, num_clients=number_of_clients, alpha=dirichlet_alpha\n",
    "    )\n",
    "\n",
    "    # Optional: example heterogeneity profile\n",
    "    # hetero_profiles = {\n",
    "    #     0: {\"epochs\": 2, \"batch_size\": 32, \"lr\": 0.02},\n",
    "    #     3: {\"epochs\": 1, \"batch_size\": 128, \"lr\": 0.005},\n",
    "    # }\n",
    "    hetero_profiles = None\n",
    "\n",
    "    # --- Federated training ---\n",
    "    model, hist = federated_training(\n",
    "        train_dataset=train_dataset,\n",
    "        test_dataset=test_dataset,\n",
    "        partitions=partitions,\n",
    "        rounds=number_of_rounds,\n",
    "        local_epochs=local_epochs,\n",
    "        device=device_pref or \"auto\",\n",
    "        q=participation,\n",
    "        num_classes=100,\n",
    "        batch_size=batch_size,\n",
    "        lr=lr,\n",
    "        hetero_profiles=hetero_profiles,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
